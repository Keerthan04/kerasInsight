from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List
import aiohttp
import asyncio
from scraper.web_scraper import WebScraper
from database.pinecone_manager import PineconeManager
from embedding.text_embedder import TextEmbedder
from retrieval.information_retriever import InformationRetriever
from llm.ollama_responder import OllamaLLMResponder
from evaluation.newRecentEvaluation import CustomEvaluator,LocalEvaluator
from llm.llm_responder import LLMResponder
# from evaluation.rag_evaluator import RAGEvaluator
from config import API_KEY, INDEX_NAME, MODEL_NAME, OLLAMA_MODEL_NAME, OPENAI_API_KEY,GEMMA_MODEL_SAVED_DIR #use of config is like this

router = APIRouter()

class URLInput(BaseModel):
    urls: List[str]

class QueryInput(BaseModel):
    query: str
@router.get("/")
async def read_root():
    return {"message": "Hello World"}
@router.post("/scrape") #Working good slight changes in returning can be seen
async def scrape_endpoints(url_input: URLInput):
    """
    Scrape a list of URLs and store the extracted data in the Pinecone index.

    Args:
        url_input (URLInput): A list of URLs to scrape.

    Returns:
        A JSON response containing a message and the scraped data.
    """
    
    pinecone_manager = PineconeManager(API_KEY, INDEX_NAME)
    pinecone_manager.initialize_index()
    embedder = TextEmbedder(MODEL_NAME)
    
    async with aiohttp.ClientSession() as session:
        scraper = WebScraper(session, embedder, pinecone_manager.index)
        scraped_data = await scraper.scrape_urls(url_input.urls)
    
    return {"message": "Scraping completed", "data": scraped_data}

@router.post("/query")
async def query_information(query_input: QueryInput):
    #this is working now where it is taking input results retrieve and response also and showing those in the response also
    print(query_input)
    """
    This endpoint takes a query as input and returns a response. The response
    is generated by the OLLAMA model and is based on the results of the query
    from the information retrieval system.

    The response also includes the results of the query from the information
    retrieval system, as well as an evaluation of the response.

    The evaluation is not currently implemented, but it will be used to evaluate
    the response and provide feedback to the user.

    :param query_input: The query from the user
    :return: A dictionary containing the query, the results of the query from the
             information retrieval system, the response from the OLLAMA model, and
             an evaluation of the response
    """

    pinecone_manager = PineconeManager(API_KEY, INDEX_NAME)
    embedder = TextEmbedder(MODEL_NAME)
    retriever = InformationRetriever(pinecone_manager.index, embedder)
    print(query_input.query)
    results = await retriever.query_index(query_input.query)
    
    #these are ollama usage and doing
    # ollama_responder = OllamaLLMResponder(OLLAMA_MODEL_NAME)
    
    # prompt = ollama_responder.construct_prompt(results, query_input.query)
    # response = ollama_responder.generate_response(prompt)
    
    # Evaluate the response
    # evaluator = RAGEvaluator(OPENAI_API_KEY)
    # evaluation = evaluator.evaluate(query_input.query, prompt, response)
    
    #trying with the saved model of gemma
    
    llm_responder = LLMResponder(model_dir=GEMMA_MODEL_SAVED_DIR)
    prompt,context = llm_responder.construct_prompt(results=results,query=query_input)
    response = llm_responder.generate_response(prompt=prompt)
    
    #evaluating with new evaluator to test
    evaluator = CustomEvaluator()
    eval_results = evaluator.evaluate(query_input, response, context)
    
    #TODO
    #now initialize of the pinecone index everytime  in the pinecone manger so better to do is on startup shd be initialized and also that when the req and res the mistral is taking around 4 to 5 mins to respond so see that and also using diff llm and also hf llm and evaluation
    return {
        "query": query_input.query,
        # "results": results, ->for testing dont need now
        "llm_response": response,
        # "evaluation": evaluation
        "evaluation": eval_results
    }
    
    
    '''
    SCOPE FOR IMPROVEMENT BASED ON THE QUESTION OF EXPLAIN ABOUT SEQUENTIAL API AND RESPONSE AND CHUNKS RETRIEVED(SEE THE REQUEST)
    Answer Relevance (Score: 0.8/1)
Strengths: The LLM response accurately explains the Sequential API, covering essential concepts such as stacking layers linearly, using the Sequential.add() method, and comparing the Sequential API with the Functional API. The explanation is mostly clear and addresses the query about the Sequential API.

Areas for Improvement: While the response is generally accurate, it could be more concise. Some details, such as the explanation about weights being created after the first call to a training/evaluation method, might not be directly relevant to a basic understanding of the Sequential API.

2. Context Relevance (Score: 0.6/1)
Strengths: The retrieved chunks provide relevant information about the Sequential API, including code examples and descriptions of key methods like Sequential.add() and Sequential.pop(). The LLM effectively used these contexts to construct the response.

Areas for Improvement: Not all chunks are equally relevant. For example, the chunk about the Functional API (https://keras.io/api/models/model/-2) is not directly related to the query about the Sequential API, and including it in the context might have added unnecessary complexity. The context could be better filtered to focus exclusively on Sequential API-related content.

3. Groundedness (Score: 0.7/1)
Strengths: The response is mostly grounded in the provided context, with accurate references to methods like Sequential.add() and Sequential.pop(), and the concept of stacking layers is correctly presented. The response also accurately reflects the code examples from the context.

Areas for Improvement: Some parts of the response, like the comparison with the Functional API, seem to be extrapolated beyond the provided context. While the comparison is valid, itâ€™s not directly grounded in the retrieved chunks, which means the LLM might be relying on its own knowledge rather than the context provided.

Suggestions for Improvement:
Focus the Context: Ensure that the retrieved chunks are more tightly related to the query. Avoid including irrelevant chunks, like the one about the Functional API, when the query is specific to the Sequential API.

Conciseness: The LLM could be encouraged to provide more concise explanations, focusing on the most relevant details and omitting less relevant information unless specifically requested.

Groundedness: Encourage the LLM to rely more on the provided context rather than extrapolating information that may not be directly supported by the retrieved chunks.

Overall, the LLM performed reasonably well but could benefit from a more focused context and slightly more concise responses.

main focus shd be on diff ways to chunk so that we get better context and all
    '''