{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine tuning of Gemini Model for our specific task with the dataset generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### scraping of most keras pages done and most relevant 10 questions and preferred answers are taken in a json format and a dataset created for the fine tuning of the gemini pro model which is done using the google cloud services so that we can get better answers in the Generation step of rag of my application. The fine tuned model curve and loss also seen and observed that model has finetuned well to get lower mean loss for the specific questions and then asked with relevant questions for the model and seem to have got good answers also from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJieweLBwGtJ",
        "outputId": "dc7457fc-e39a-4b35-f79c-359cb1b7b025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are authorizing client libraries without access to a web browser. Please run the following command on a machine with a web browser and copy its output back here. Make sure the installed gcloud version is 372.0.0 or newer.\n",
            "\n",
            "gcloud auth application-default login --remote-bootstrap=\"https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=959496487289-g01iveci34f3u1nlufbaj2s9ccklp8o1.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgenerative-language.retriever+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgenerative-language.tuning&state=yzxI6RmQeDzwdARfqjtU2IsAJvhCAy&access_type=offline&code_challenge=6BpkVn-QYry7wywSJI1bw5TuqsAv6X_ggIxA9vvSTpY&code_challenge_method=S256&token_usage=remote\"\n",
            "\n",
            "\n",
            "Enter the output of the above command: https://localhost:8085/?state=yzxI6RmQeDzwdARfqjtU2IsAJvhCAy&code=4/0AQlEd8wtcAvjggCK4NM2w_GkLGrzy1pf2G0qtk11GNjix8iCbZy3jUoXkqTi3nRAGPFIKg&scope=https://www.googleapis.com/auth/cloud-platform%20https://www.googleapis.com/auth/generative-language.retriever%20https://www.googleapis.com/auth/generative-language.tuning\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth application-default login --no-browser --client-id-file=client_secret.json --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever,https://www.googleapis.com/auth/generative-language.tuning'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMF1PX7IwYWC",
        "outputId": "c3d769da-6c81-4653-9d47-a66e02127743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.23.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1BYYvqlxmmx",
        "outputId": "02d78128-000c-4439-9066-88103d532a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available base models: ['models/chat-bison-001', 'models/text-bison-001', 'models/embedding-gecko-001', 'models/gemini-1.0-pro-latest', 'models/gemini-1.0-pro', 'models/gemini-pro', 'models/gemini-1.0-pro-001', 'models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-001', 'models/gemini-1.5-pro', 'models/gemini-1.5-pro-exp-0801', 'models/gemini-1.5-pro-exp-0827', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash-001', 'models/gemini-1.5-flash-001-tuning', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-exp-0827', 'models/gemini-1.5-flash-8b-exp-0827', 'models/embedding-001', 'models/text-embedding-004', 'models/aqa']\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "print('Available base models:', [m.name for m in genai.list_models()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iSbN42qxti8",
        "outputId": "051f983a-373b-40bf-dccc-aeb9b931698a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tunedModels/increment-5aogvgj0bbjf\n",
            "tunedModels/increment-9cj8gbmi8ahh\n",
            "tunedModels/ftgemmafordlrandomlr00005bs16ec10-xgo8a9\n",
            "tunedModels/ftgfordllr00001bs8ec50-kzo5by6qiujl\n"
          ]
        }
      ],
      "source": [
        "for model_info in genai.list_tuned_models():\n",
        "    print(model_info.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3nvLvan1n5v",
        "outputId": "28971c21-c94f-49ef-eac0-a916a5edd967"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TunedModel(name='tunedModels/ftgfordllr00001bs8ec50-kzo5by6qiujl',\n",
              "           source_model='models/gemini-1.5-flash-001-tuning',\n",
              "           base_model='models/gemini-1.5-flash-001-tuning',\n",
              "           display_name='FT-Gfordl-lr0.0001-bs8-ec50',\n",
              "           description='',\n",
              "           temperature=1.0,\n",
              "           top_p=0.95,\n",
              "           top_k=64,\n",
              "           state=<State.CREATING: 1>,\n",
              "           create_time=datetime.datetime(2024, 9, 14, 14, 19, 48, 681714, tzinfo=datetime.timezone.utc),\n",
              "           update_time=datetime.datetime(2024, 9, 14, 14, 19, 48, 681714, tzinfo=datetime.timezone.utc),\n",
              "           tuning_task=TuningTask(start_time=None,\n",
              "                                  complete_time=None,\n",
              "                                  snapshots=[],\n",
              "                                  hyperparameters=Hyperparameters(epoch_count=50,\n",
              "                                                                  batch_size=8,\n",
              "                                                                  learning_rate=0.0001)))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.get_tuned_model('tunedModels/ftgfordllr00001bs8ec50-kzo5by6qiujl')\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKRM5sdq1uhC",
        "outputId": "ef11c0c7-2902-471f-c080-e62efdca1e7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<State.CREATING: 1>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ySjA1nG7Xze3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YdF6SolSx5_G"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    {\n",
        "        \"text_input\": \"What is the Model class in Keras?\",\n",
        "        \"output\": \"The Model class in Keras is an object that groups layers and provides features for training and inference. It can be instantiated using the Functional API, by subclassing the Model class, or using the Sequential class.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you create a model using the Functional API in Keras?\",\n",
        "        \"output\": \"To create a model using the Functional API in Keras, you start by defining input tensors using `keras.Input()`, then chain layer calls to specify the forward pass, and finally, create the model using `keras.Model(inputs=inputs, outputs=outputs)`. For example:\\n\\n```python\\ninputs = keras.Input(shape=(37,))\\nx = keras.layers.Dense(32, activation=\\\"relu\\\")(inputs)\\noutputs = keras.layers.Dense(5, activation=\\\"softmax\\\")(x)\\nmodel = keras.Model(inputs=inputs, outputs=outputs)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"Can you use nested inputs when creating models in the Functional API?\",\n",
        "        \"output\": \"No, nested inputs are not supported in the Functional API. Only dicts, lists, and tuples of input tensors are supported. For example, lists of lists or dicts of dicts are not allowed.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you create sub-models from a larger Functional API model?\",\n",
        "        \"output\": \"You can create sub-models by using intermediate tensors from the main model. These sub-models will share layers and weights with the full model. For example:\\n\\n```python\\ninputs = keras.Input(shape=(None, None, 3))\\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\\npooling = keras.layers.GlobalAveragePooling2D()(conv)\\nfeature = keras.layers.Dense(10)(pooling)\\n\\nfull_model = keras.Model(inputs, feature)\\nbackbone = keras.Model(processed, conv)\\nactivations = keras.Model(conv, feature)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you create a custom model by subclassing the Model class in Keras?\",\n",
        "        \"output\": \"To create a custom model by subclassing the Model class in Keras, you define the layers in the `__init__()` method and implement the forward pass in the `call()` method. For example:\\n\\n```python\\nclass MyModel(keras.Model):\\n    def __init__(self):\\n        super().__init__()\\n        self.dense1 = keras.layers.Dense(32, activation=\\\"relu\\\")\\n        self.dense2 = keras.layers.Dense(5, activation=\\\"softmax\\\")\\n\\n    def call(self, inputs):\\n        x = self.dense1(inputs)\\n        return self.dense2(x)\\n\\nmodel = MyModel()\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you specify different behavior during training and inference when subclassing the Model class?\",\n",
        "        \"output\": \"When subclassing the Model class, you can add a `training` argument to the `call()` method to specify different behavior for training and inference. For example:\\n\\n```python\\nclass MyModel(keras.Model):\\n    def __init__(self):\\n        super().__init__()\\n        self.dense1 = keras.layers.Dense(32, activation=\\\"relu\\\")\\n        self.dense2 = keras.layers.Dense(5, activation=\\\"softmax\\\")\\n        self.dropout = keras.layers.Dropout(0.5)\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense1(inputs)\\n        x = self.dropout(x, training=training)\\n        return self.dense2(x)\\n\\nmodel = MyModel()\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the Sequential class in Keras?\",\n",
        "        \"output\": \"The Sequential class in Keras is a special case of the Model class where the model is a linear stack of layers with single input and single output. Here's an example:\\n\\n```python\\nmodel = keras.Sequential([\\n    keras.Input(shape=(None, None, 3)),\\n    keras.layers.Conv2D(filters=32, kernel_size=3),\\n])\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you retrieve a specific layer from a Keras model?\",\n",
        "        \"output\": \"You can retrieve a layer from a Keras model using `model.get_layer()`. You can specify either the name or the index of the layer. If both are provided, the index takes precedence. For example:\\n\\n```python\\nlayer = model.get_layer(name=\\\"conv2d\\\")\\n# or\\nlayer = model.get_layer(index=2)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you print a summary of a Keras model?\",\n",
        "        \"output\": \"You can print a summary of a Keras model using `model.summary()`. You can customize the output by adjusting parameters like `line_length`, `positions`, `expand_nested`, `show_trainable`, and `layer_range`. For example:\\n\\n```python\\nmodel.summary(line_length=80, show_trainable=True)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the difference between the Functional API and Sequential API in Keras?\",\n",
        "        \"output\": \"The Functional API allows you to create models with complex architectures, such as multi-input/output models or models with shared layers. The Sequential API is simpler and only allows for linear stacking of layers, where each layer has a single input and single output. The Functional API provides more flexibility, while the Sequential API is more straightforward for simple models.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the Sequential class in Keras?\",\n",
        "        \"output\": \"The Sequential class in Keras groups a linear stack of layers into a Model. It allows for a straightforward approach to building models where layers are added sequentially.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you create a Sequential model in Keras?\",\n",
        "        \"output\": \"You can create a Sequential model in Keras by initializing it with `keras.Sequential()` and then adding layers to it using `model.add()`. Here's an example:\\n\\n```python\\nmodel = keras.Sequential()\\nmodel.add(keras.Input(shape=(16,)))\\nmodel.add(keras.layers.Dense(8))\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"Do you need to specify an input shape when creating a Sequential model?\",\n",
        "        \"output\": \"No, specifying an input shape is optional. If you do not specify an `Input`, the model won't have weights until you call a training or evaluation method. When you do provide an `Input`, the model gets built continuously as layers are added.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What happens if you don't specify an input shape in a Sequential model?\",\n",
        "        \"output\": \"If you don't specify an input shape, the model's weights are not created until you call a training, evaluation, or prediction method for the first time. You can also manually build the model by calling `model.build(batch_input_shape)`.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you manually build a Sequential model in Keras?\",\n",
        "        \"output\": \"You can manually build a Sequential model by calling `model.build(batch_input_shape)` after adding layers. For example:\\n\\n```python\\nmodel = keras.Sequential()\\nmodel.add(keras.layers.Dense(8))\\nmodel.add(keras.layers.Dense(4))\\nmodel.build((None, 16))\\nlen(model.weights)  # Returns 4\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the `add` method in the Sequential class?\",\n",
        "        \"output\": \"The `add()` method in the Sequential class adds a layer instance on top of the layer stack. For example:\\n\\n```python\\nmodel = keras.Sequential()\\nmodel.add(keras.layers.Dense(8))\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you remove a layer from a Sequential model?\",\n",
        "        \"output\": \"You can remove the last layer from a Sequential model using the `pop()` method. For example:\\n\\n```python\\nmodel = keras.Sequential()\\nmodel.add(keras.layers.Dense(8))\\nmodel.add(keras.layers.Dense(4))\\nmodel.pop()  # Removes the Dense(4) layer\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What happens when you compile a Sequential model without building it first?\",\n",
        "        \"output\": \"If you compile a Sequential model without specifying an input shape, it gets built for the first time when you call `fit()`, `evaluate()`, or `predict()` methods. For example:\\n\\n```python\\nmodel = keras.Sequential()\\nmodel.add(keras.layers.Dense(8))\\nmodel.add(keras.layers.Dense(1))\\nmodel.compile(optimizer='sgd', loss='mse')\\nmodel.fit(x, y, batch_size=32, epochs=10)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"Can you specify the input shape later when building a Sequential model?\",\n",
        "        \"output\": \"Yes, you can specify the input shape later when building the Sequential model by using `model.build(batch_input_shape)`. This allows for delayed building of the model.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the Sequential model in Keras?\",\n",
        "        \"output\": \"The purpose of the Sequential model in Keras is to provide a simple way to stack layers in a linear sequence. It is ideal for models where each layer has only one input and one output.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the Model.compile() method do, and what are its key arguments?\",\n",
        "        \"output\": \"The `Model.compile()` method configures the model for training by specifying the optimizer, loss function, and metrics to be used. Key arguments include: \\n- `optimizer`: Defines the optimization algorithm.\\n- `loss`: Specifies the loss function to minimize.\\n- `metrics`: Defines the metrics to evaluate during training and testing. For example:\\n\\n```python\\nmodel.compile(\\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\\n    loss=keras.losses.BinaryCrossentropy(),\\n    metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\\n)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you specify different loss functions for multi-output models in Model.compile()?\",\n",
        "        \"output\": \"For models with multiple outputs, you can pass a dictionary or a list of loss functions to `Model.compile()`. Each output will have its own loss function. For example:\\n\\n```python\\nmodel.compile(\\n    optimizer='adam',\\n    loss={'output_1': 'sparse_categorical_crossentropy', 'output_2': 'mse'},\\n    metrics={'output_1': 'accuracy'}\\n)\\n```\\nHere, `output_1` uses `sparse_categorical_crossentropy`, and `output_2` uses `mse` (mean squared error).\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the metrics argument in Model.compile()?\",\n",
        "        \"output\": \"The `metrics` argument in `Model.compile()` is used to specify which metrics to monitor during training and evaluation. It can include standard metrics like `accuracy`, or custom metrics such as precision or recall. For instance:\\n\\n```python\\nmodel.compile(\\n    optimizer='adam',\\n    loss='binary_crossentropy',\\n    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\\n)\\n```\\nThis tracks both accuracy and precision/recall during training.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you customize the learning rate for a specific optimizer in Keras?\",\n",
        "        \"output\": \"To customize the learning rate for a specific optimizer in Keras, you can pass the `learning_rate` argument when creating the optimizer. For example, with the Adam optimizer:\\n\\n```python\\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\\nmodel.compile(optimizer=optimizer, loss='mse')\\n```\\nIn this case, the learning rate is set to `0.001` for the Adam optimizer.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the difference between a built-in metric and a custom metric in Keras?\",\n",
        "        \"output\": \"A built-in metric is a predefined metric such as `accuracy`, `precision`, or `mean_squared_error` that Keras provides. A custom metric is defined by the user and can be created using a function that computes the metric based on the true and predicted values. For example, a custom metric for computing mean absolute error (MAE):\\n\\n```python\\ndef custom_mae(y_true, y_pred):\\n    return tf.reduce_mean(tf.abs(y_pred - y_true))\\n\\nmodel.compile(optimizer='adam', loss='mse', metrics=[custom_mae])\\n```\\nHere, `custom_mae` is used as a metric during model training.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you use a custom loss function in Keras?\",\n",
        "        \"output\": \"You can define a custom loss function by creating a Python function that takes `y_true` and `y_pred` as arguments, and then returns a scalar loss value. For example, a custom loss function for mean absolute percentage error (MAPE):\\n\\n```python\\ndef custom_mape(y_true, y_pred):\\n    diff = tf.abs((y_true - y_pred) / tf.clip_by_value(y_true, 1e-7, None))\\n    return 100 * tf.reduce_mean(diff)\\n\\nmodel.compile(optimizer='adam', loss=custom_mape)\\n```\\nThis custom loss can then be passed to `Model.compile()`.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What are callbacks in Keras, and how can they be used during training?\",\n",
        "        \"output\": \"Callbacks in Keras are functions or objects that are called during different stages of training (such as at the end of an epoch or batch). They allow you to monitor and modify the training process dynamically. Common callbacks include `EarlyStopping`, `ModelCheckpoint`, and `ReduceLROnPlateau`. For example:\\n\\n```python\\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\\nmodel.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, callbacks=[callback])\\n```\\nHere, training stops early if validation loss doesn't improve for 3 consecutive epochs.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you fine-tune a pre-trained model in Keras?\",\n",
        "        \"output\": \"To fine-tune a pre-trained model in Keras, you typically follow these steps:\\n1. Load a pre-trained model with frozen layers.\\n2. Add new layers on top (for the specific task).\\n3. Compile and train the new layers on your dataset.\\n4. Unfreeze some or all of the pre-trained layers and continue training with a lower learning rate.\\n\\n```python\\nbase_model = keras.applications.MobileNetV2(weights='imagenet', include_top=False)\\nbase_model.trainable = False  # Freeze the base model\\nmodel = keras.Sequential([\\n    base_model,\\n    keras.layers.GlobalAveragePooling2D(),\\n    keras.layers.Dense(10, activation='softmax')\\n])\\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\\n# After initial training, unfreeze the base model and fine-tune\\nbase_model.trainable = True\\nmodel.compile(optimizer=keras.optimizers.Adam(1e-5), loss='sparse_categorical_crossentropy')\\n```\\nHere, `MobileNetV2` is fine-tuned on a new dataset.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is model checkpointing, and how can you implement it in Keras?\",\n",
        "        \"output\": \"Model checkpointing in Keras allows you to save the model at various stages during training, such as after every epoch or when a certain condition is met. It helps to save the best model (e.g., the one with the lowest validation loss). Here's how you implement it:\\n\\n```python\\ncheckpoint = keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\\nmodel.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, callbacks=[checkpoint])\\n```\\nIn this example, the model with the lowest validation loss is saved as `best_model.h5`.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you evaluate a trained Keras model on a test dataset?\",\n",
        "        \"output\": \"You can evaluate a trained Keras model using the `evaluate()` method. This method returns the loss and any metrics specified during compilation. For example:\\n\\n```python\\ntest_loss, test_acc = model.evaluate(x_test, y_test)\\nprint(f'Test accuracy: {test_acc}')\\n```\\nIn this example, the model's performance on the test dataset is evaluated, and the test accuracy is printed.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the Keras Layers API in neural network construction?\",\n",
        "        \"output\": \"The Keras Layers API provides the basic building blocks for constructing neural networks. Each layer represents a tensor-in tensor-out computation function, defined by the `call` method, and manages its own state through TensorFlow variables (weights). Layers can be called with input data and return output data, making them fundamental components in designing and training models. For example:\\n\\n```python\\nfrom keras import layers\\n\\nlayer = layers.Dense(32, activation='relu')\\ninputs = keras.random.uniform(shape=(10, 20))\\noutputs = layer(inputs)\\n```\\nIn this snippet, `layers.Dense` is used to create a dense layer with 32 units and ReLU activation.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you use a Keras layer instance to process input data?\",\n",
        "        \"output\": \"To use a Keras layer instance for processing input data, you first create an instance of the layer (e.g., `layers.Dense`). Then, you call the layer instance with the input data. The layer processes the data according to its computation function (defined in the `call` method) and returns the output. For example:\\n\\n```python\\nfrom keras import layers\\n\\nlayer = layers.Dense(32, activation='relu')\\ninputs = keras.random.uniform(shape=(10, 20))\\noutputs = layer(inputs)\\n```\\nHere, `layer(inputs)` applies the dense layer to the input data, producing the `outputs`.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you access the weights of a Keras layer after it has processed data?\",\n",
        "        \"output\": \"After a Keras layer has processed data, you can access its weights through the `weights` attribute of the layer instance. This attribute returns a list of the layer's weight variables. For example:\\n\\n```python\\nfrom keras import layers\\n\\nlayer = layers.Dense(32, activation='relu')\\ninputs = keras.random.uniform(shape=(10, 20))\\noutputs = layer(inputs)\\nprint(layer.weights)\\n```\\nThe `layer.weights` output might look like:\\n\\n```python\\n[<KerasVariable shape=(20, 32), dtype=float32, path=dense/kernel>, <KerasVariable shape=(32,), dtype=float32, path=dense/bias>]\\n```\\nThis list contains the weights and biases of the dense layer.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can activations be used with Keras layers?\",\n",
        "        \"output\": \"Activations can be used with Keras layers either by passing them as a parameter to the `activation` argument when creating a layer, or by using the `Activation` layer separately. For example:\\n\\n```python\\nfrom keras import layers, activations\\n\\n# Using activation argument\\nmodel.add(layers.Dense(64, activation=activations.relu))\\n\\n# Using Activation layer\\nmodel.add(layers.Dense(64))\\nmodel.add(layers.Activation(activations.relu))\\n```\\nBoth approaches are equivalent.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the ReLU activation function do?\",\n",
        "        \"output\": \"The ReLU (Rectified Linear Unit) activation function applies the operation `max(x, 0)` to each element of the input tensor. It outputs 0 for any negative input and the input value itself for any positive input. For example:\\n\\n```python\\nimport keras\\nx = [-10, -5, 0.0, 5, 10]\\nprint(keras.activations.relu(x))  # Output: [ 0.,  0.,  0.,  5., 10.]\\n```\\nYou can also adjust parameters like `negative_slope`, `max_value`, and `threshold` to modify its behavior.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does the sigmoid activation function work?\",\n",
        "        \"output\": \"The sigmoid activation function computes `sigmoid(x) = 1 / (1 + exp(-x))`. It maps input values to a range between 0 and 1. This function is often used for binary classification tasks. For example:\\n\\n```python\\nimport keras\\nx = [-10, 0, 10]\\nprint(keras.activations.sigmoid(x))  # Output: [4.539787e-05, 0.5, 0.9999546]\\n```\\nThe sigmoid function is useful for output layers in binary classification.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the softmax activation function?\",\n",
        "        \"output\": \"The softmax activation function converts a vector of values into a probability distribution. Each element of the output vector is in the range [0, 1] and the elements sum to 1. It is commonly used in the final layer of classification models to interpret outputs as probabilities. For example:\\n\\n```python\\nimport keras\\nimport numpy as np\\nx = np.array([1.0, 2.0, 3.0])\\nprint(keras.activations.softmax(x))  # Output: [0.09003057, 0.24472847, 0.66524096]\\n```\\nThis makes it suitable for multi-class classification tasks.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the softplus activation function do?\",\n",
        "        \"output\": \"The softplus activation function is defined as `softplus(x) = log(exp(x) + 1)`. It is a smooth approximation of the ReLU function. For example:\\n\\n```python\\nimport keras\\nx = [-10, 0, 10]\\nprint(keras.activations.softplus(x))  # Output: [4.53988992e-05, 6.93147181e-01, 1.00004540e+01]\\n```\\nIt provides a non-zero gradient for negative inputs, unlike ReLU.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How is the softsign activation function defined?\",\n",
        "        \"output\": \"The softsign activation function is defined as `softsign(x) = x / (abs(x) + 1)`. It provides smoother outputs compared to ReLU and has outputs in the range (-1, 1). For example:\\n\\n```python\\nimport keras\\nx = [-10, 0, 10]\\nprint(keras.activations.softsign(x))  # Output: [-0.90909091, 0., 0.90909091]\\n```\\nIt is less aggressive than ReLU and can be useful in various contexts.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the SELU activation function?\",\n",
        "        \"output\": \"The SELU (Scaled Exponential Linear Unit) activation function is designed to self-normalize activations. It is defined as:\\n\\n```python\\nx if x > 0\\nalpha * exp(x) - 1 if x < 0\\n```\\nwhere `alpha` and `scale` are pre-defined constants. SELU helps maintain the mean and variance of the inputs, which can be beneficial for deep networks. For example:\\n\\n```python\\nimport keras\\nx = [-1, 0, 1]\\nprint(keras.activations.selu(x))\\n```\\nSELUs are used with the LecunNormal initializer and AlphaDropout.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the ELU activation function do?\",\n",
        "        \"output\": \"The ELU (Exponential Linear Unit) activation function is defined as:\\n\\n```python\\nx if x > 0\\nalpha * exp(x) - 1 if x < 0\\n```\\nELUs have negative values which push the mean of activations closer to zero, helping with faster learning. For example:\\n\\n```python\\nimport keras\\nx = [-1, 0, 1]\\nprint(keras.activations.elu(x, alpha=1.0))\\n```\\nELUs provide a smoother transition for negative inputs compared to ReLU.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How is the leaky ReLU activation function different from ReLU?\",\n",
        "        \"output\": \"The leaky ReLU activation function is defined as `leaky_relu(x, negative_slope=0.2)`. Unlike ReLU, which outputs 0 for negative inputs, leaky ReLU allows a small, non-zero gradient for negative inputs defined by `negative_slope`. For example:\\n\\n```python\\nimport keras\\nx = [-10, 0, 10]\\nprint(keras.activations.leaky_relu(x, negative_slope=0.2))  # Output: [-2., 0., 10.]\\n```\\nThis prevents dead neurons during training.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the difference between the swish and hard swish activation functions?\",\n",
        "        \"output\": \"The swish (or SiLU) activation function is defined as `swish(x) = x * sigmoid(x)`, providing a smooth, non-monotonic activation. For example:\\n\\n```python\\nimport keras\\nx = [-1, 0, 1]\\nprint(keras.activations.silu(x))\\n```\\nThe hard swish function is a piecewise linear approximation of swish:\\n\\n```python\\n0 if x < -3\\nx if x > 3\\nx * (x + 3) / 6 if -3 <= x <= 3\\n```\\nThis makes hard swish faster to compute but less smooth.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the GELU activation function do?\",\n",
        "        \"output\": \"The GELU (Gaussian Error Linear Unit) activation function is defined as `gelu(x) = x * P(X <= x)`, where `P(X)` is the cumulative distribution function of a standard normal distribution. For example:\\n\\n```python\\nimport keras\\nx = [-1, 0, 1]\\nprint(keras.activations.gelu(x, approximate=True))\\n```\\nGELU is useful for probabilistic modeling as it weights inputs by their value, not just their sign.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can custom activations be created in Keras?\",\n",
        "        \"output\": \"Custom activations in Keras can be created by defining a function that takes a tensor as input and returns a tensor of the same shape and dtype. This custom function can then be used as an activation function in a layer. For example:\\n\\n```python\\nimport keras\\nimport tensorflow as tf\\n\\ndef custom_activation(x):\\n    return tf.math.tanh(x)  # Example custom activation function\\n\\nmodel.add(layers.Dense(64, activation=custom_activation))\\n```\\nHere, `custom_activation` is used as a custom activation function for a Dense layer.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the Conv2D layer do in a neural network?\",\n",
        "        \"output\": \"The Conv2D layer creates a convolution kernel that is convolved with the input tensor over 2D spatial dimensions (height and width) to produce an output tensor. It applies the convolution operation to detect features from the input image or spatial data.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import Conv2D\\n\\nx = np.random.rand(4, 10, 10, 128)\\nconv_layer = Conv2D(32, 3, activation='relu')\\ny = conv_layer(x)\\nprint(y.shape)\\n# Output: (4, 8, 8, 32)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How is the kernel_size argument used in the Conv2D layer?\",\n",
        "        \"output\": \"The kernel_size argument specifies the size of the convolution window. It can be an integer (for a square kernel) or a tuple/list of two integers (for a rectangular kernel). This defines the dimensions of the filter used in the convolution operation.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import Conv2D\\n\\nconv_layer = Conv2D(32, kernel_size=(3, 5), activation='relu')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the padding argument do in the Conv2D layer?\",\n",
        "        \"output\": \"The padding argument determines how the input is padded before applying the convolution. If set to \\\"valid\\\", no padding is applied. If set to \\\"same\\\", padding is applied such that the output has the same spatial dimensions as the input when strides are set to 1.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import Conv2D\\n\\nconv_layer_valid = Conv2D(32, kernel_size=3, padding='valid')\\nconv_layer_same = Conv2D(32, kernel_size=3, padding='same')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What are the possible values for the data_format argument in the Conv2D layer, and what do they signify?\",\n",
        "        \"output\": \"The data_format argument can be either \\\"channels_last\\\" or \\\"channels_first\\\". \\\"channels_last\\\" indicates that the input shape is (batch_size, height, width, channels), while \\\"channels_first\\\" means the input shape is (batch_size, channels, height, width). It specifies the ordering of dimensions in the input tensor.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import Conv2D\\n\\nconv_layer_last = Conv2D(32, kernel_size=3, data_format='channels_last')\\nconv_layer_first = Conv2D(32, kernel_size=3, data_format='channels_first')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the activation argument in the Conv2D layer?\",\n",
        "        \"output\": \"The activation argument specifies the activation function to be applied to the output of the convolution operation. If None, no activation function is applied, and the layer output is just the convolution result plus any bias term.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import Conv2D\\nfrom keras.activations import relu\\n\\nconv_layer = Conv2D(32, kernel_size=3, activation=relu)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the dilation_rate argument control in the Conv2D layer?\",\n",
        "        \"output\": \"The dilation_rate argument specifies the dilation rate for dilated convolution. It can be an integer or a tuple/list of two integers. This parameter controls the spacing between kernel elements, effectively allowing the kernel to cover a larger area of the input.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import Conv2D\\n\\nconv_layer = Conv2D(32, kernel_size=3, dilation_rate=2)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What happens if both strides and dilation_rate are set to values greater than 1 in the Conv2D layer?\",\n",
        "        \"output\": \"If both strides and dilation_rate are set to values greater than 1, it will raise a ValueError. Strides and dilation rates are mutually exclusive in the sense that you cannot use them simultaneously in the Conv2D layer.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import Conv2D\\n\\n# This will raise a ValueError\\nconv_layer = Conv2D(32, kernel_size=3, strides=2, dilation_rate=2)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the MaxPooling2D layer do in a neural network?\",\n",
        "        \"output\": \"The MaxPooling2D layer performs a downsampling operation on 2D spatial data by selecting the maximum value within a defined window (specified by pool_size) for each channel of the input. This reduces the spatial dimensions of the input while retaining the most important features.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import MaxPooling2D\\n\\nx = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\\nx = np.reshape(x, [1, 3, 3, 1])\\nmax_pool_2d = MaxPooling2D(pool_size=(2, 2))\\noutput = max_pool_2d(x)\\nprint(output.shape)\\n# Output: (1, 2, 2, 1)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How is the pool_size argument used in the MaxPooling2D layer?\",\n",
        "        \"output\": \"The pool_size argument defines the size of the window that will be used to perform the max pooling operation. It can be an integer (for a square window) or a tuple of two integers (for a rectangular window). This parameter determines how much the input is downsampled.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import MaxPooling2D\\n\\nmax_pool_2d = MaxPooling2D(pool_size=(2, 2))\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the strides argument do in the MaxPooling2D layer?\",\n",
        "        \"output\": \"The strides argument specifies how the pooling window is moved across the input. It can be an integer (for uniform strides in both dimensions) or a tuple of two integers (for different strides in each dimension). If not specified, it defaults to the pool_size value.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import MaxPooling2D\\n\\nmax_pool_2d = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the padding argument do in the MaxPooling2D layer?\",\n",
        "        \"output\": \"The padding argument determines how the input is padded before applying the max pooling operation. If set to \\\"valid\\\", no padding is applied, and only complete windows are used for pooling. If set to \\\"same\\\", padding is applied such that the output has the same height and width dimensions as the input when strides are set to 1.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import MaxPooling2D\\n\\nmax_pool_2d_valid = MaxPooling2D(pool_size=(2, 2), padding='valid')\\nmax_pool_2d_same = MaxPooling2D(pool_size=(2, 2), padding='same')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What are the possible values for the data_format argument in the MaxPooling2D layer, and what do they signify?\",\n",
        "        \"output\": \"The data_format argument can be either \\\"channels_last\\\" or \\\"channels_first\\\". \\\"channels_last\\\" indicates that the input shape is (batch_size, height, width, channels), while \\\"channels_first\\\" means the input shape is (batch_size, channels, height, width). This specifies the ordering of dimensions in the input tensor.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import MaxPooling2D\\n\\nmax_pool_2d_last = MaxPooling2D(pool_size=(2, 2), data_format='channels_last')\\nmax_pool_2d_first = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the resulting output shape of the MaxPooling2D layer when using strides=(1, 1) and padding=\\\"valid\\\"?\",\n",
        "        \"output\": \"With strides=(1, 1) and padding=\\\"valid\\\", the output shape is calculated as: output_shape = math.floor((input_shape - pool_size) / strides) + 1. The pooling operation will be applied with no padding, so the output dimensions are reduced by the pool_size.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import MaxPooling2D\\n\\nx = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\\nx = np.reshape(x, [1, 3, 3, 1])\\nmax_pool_2d = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid')\\noutput = max_pool_2d(x)\\nprint(output.shape)\\n# Output: (1, 2, 2, 1)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the resulting output shape of the MaxPooling2D layer when using strides=(2, 2) and padding=\\\"valid\\\"?\",\n",
        "        \"output\": \"With strides=(2, 2) and padding=\\\"valid\\\", the output shape is calculated as: output_shape = math.floor((input_shape - pool_size) / strides) + 1. The pooling window will be shifted by 2 units in both dimensions, resulting in a reduced output size.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import MaxPooling2D\\n\\nx = np.array([[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.]])\\nx = np.reshape(x, [1, 3, 4, 1])\\nmax_pool_2d = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\\noutput = max_pool_2d(x)\\nprint(output.shape)\\n# Output: (1, 2, 2, 1)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the resulting output shape of the MaxPooling2D layer when using strides=(1, 1) and padding=\\\"same\\\"?\",\n",
        "        \"output\": \"With strides=(1, 1) and padding=\\\"same\\\", the output shape is calculated as: output_shape = math.floor((input_shape - 1) / strides) + 1. The padding ensures that the output has the same spatial dimensions as the input when strides are set to 1.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import MaxPooling2D\\n\\nx = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\\nx = np.reshape(x, [1, 3, 3, 1])\\nmax_pool_2d = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')\\noutput = max_pool_2d(x)\\nprint(output.shape)\\n# Output: (1, 3, 3, 1)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the GlobalAveragePooling2D layer do in a neural network?\",\n",
        "        \"output\": \"The GlobalAveragePooling2D layer computes the average of all values in each feature map of the input and outputs a single value per feature map. This is useful for downsampling without adding additional parameters and for reducing the spatial dimensions of the input to a single value per feature map.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import GlobalAveragePooling2D\\n\\nx = np.random.rand(2, 4, 5, 3)\\ny = GlobalAveragePooling2D()(x)\\nprint(y.shape)\\n# Output: (2, 3)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does the data_format argument affect the GlobalAveragePooling2D layer?\",\n",
        "        \"output\": \"The data_format argument determines how the dimensions of the input are ordered. If data_format is 'channels_last', the input is expected to have the shape (batch_size, height, width, channels). If data_format is 'channels_first', the input should be in the shape (batch_size, channels, height, width). The default value is usually 'channels_last', unless otherwise specified in the Keras config.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import GlobalAveragePooling2D\\n\\n# Using channels_last format (default)\\nlayer = GlobalAveragePooling2D(data_format='channels_last')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What happens when the keepdims argument is set to True in the GlobalAveragePooling2D layer?\",\n",
        "        \"output\": \"When keepdims is set to True, the spatial dimensions of the output are retained with length 1, meaning the output will have four dimensions, but the height and width will be 1. This is useful if you need to keep the dimensions consistent for further operations in the model.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import GlobalAveragePooling2D\\n\\nx = np.random.rand(2, 4, 5, 3)\\ny = GlobalAveragePooling2D(keepdims=True)(x)\\nprint(y.shape)\\n# Output: (2, 1, 1, 3)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the primary function of the LSTM layer in a neural network?\",\n",
        "        \"output\": \"The LSTM (Long Short-Term Memory) layer is used to model and learn long-term dependencies in sequential data. It is designed to handle the vanishing gradient problem often encountered in standard RNNs by maintaining a memory cell that can store information over long sequences. This allows it to learn temporal patterns and relationships in the data.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import LSTM\\n\\ninputs = np.random.random((32, 10, 8))\\nlstm = LSTM(4)\\noutput = lstm(inputs)\\nprint(output.shape)\\n# Output: (32, 4)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the return_sequences argument do in the LSTM layer?\",\n",
        "        \"output\": \"The `return_sequences` argument determines whether the LSTM layer returns the output for each time step in the input sequence or only the output from the last time step. When `return_sequences` is set to True, the output will have the shape (batch_size, timesteps, units), whereas when it is False, the output will have the shape (batch_size, units).\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import LSTM\\n\\ninputs = np.random.random((32, 10, 8))\\nlstm = LSTM(4, return_sequences=True)\\noutput = lstm(inputs)\\nprint(output.shape)\\n# Output: (32, 10, 4)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does the use_cudnn argument affect the LSTM layer's performance?\",\n",
        "        \"output\": \"The `use_cudnn` argument determines whether to use a cuDNN-based implementation of the LSTM layer, which can provide significant performance improvements on supported hardware (GPUs). If set to 'auto', the LSTM layer will use cuDNN if all requirements are met; otherwise, it will use a fallback implementation. The requirements for cuDNN include specific activation functions and no dropout.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import LSTM\\n\\nlstm = LSTM(4, use_cudnn='auto')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the dropout and recurrent_dropout arguments in the LSTM layer?\",\n",
        "        \"output\": \"The `dropout` argument specifies the fraction of the units to drop during the linear transformation of the inputs, while `recurrent_dropout` specifies the fraction of the units to drop during the linear transformation of the recurrent state. These arguments are used to prevent overfitting by randomly dropping units during training.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import LSTM\\n\\ninputs = np.random.random((32, 10, 8))\\nlstm = LSTM(4, dropout=0.2, recurrent_dropout=0.2)\\noutput = lstm(inputs)\\nprint(output.shape)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the stateful argument do in the LSTM layer?\",\n",
        "        \"output\": \"The `stateful` argument, when set to True, allows the LSTM layer to maintain the state between batches. This means that the hidden state and cell state from one batch will be used as the initial state for the next batch. This is useful for tasks where sequences are split across multiple batches but should be treated as a continuous sequence.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import LSTM\\n\\ninputs = np.random.random((32, 10, 8))\\nlstm = LSTM(4, stateful=True)\\noutput = lstm(inputs)\\nprint(output.shape)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the primary function of the GRU layer in a neural network?\",\n",
        "        \"output\": \"The GRU (Gated Recurrent Unit) layer is designed to capture and learn temporal dependencies in sequential data. It is a type of recurrent neural network layer that simplifies the traditional LSTM by using fewer gates, which can lead to fewer parameters and faster computation while still addressing the vanishing gradient problem.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import GRU\\n\\ninputs = np.random.random((32, 10, 8))\\ngru = GRU(4)\\noutput = gru(inputs)\\nprint(output.shape)\\n# Output: (32, 4)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does the return_sequences argument affect the output of the GRU layer?\",\n",
        "        \"output\": \"The `return_sequences` argument determines whether the GRU layer returns the output for each time step in the input sequence or just the output from the last time step. When `return_sequences` is set to True, the output will have the shape (batch_size, timesteps, units), whereas when it is False, the output will have the shape (batch_size, units).\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import GRU\\n\\ninputs = np.random.random((32, 10, 8))\\ngru = GRU(4, return_sequences=True)\\noutput = gru(inputs)\\nprint(output.shape)\\n# Output: (32, 10, 4)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the reset_after argument in the GRU layer?\",\n",
        "        \"output\": \"The `reset_after` argument determines whether the reset gate is applied before or after the matrix multiplication in the GRU layer. When `reset_after` is set to True, the reset gate is applied after the matrix multiplication, which is compatible with cuDNNGRU (GPU-only) and allows for inference on CPU. This setting is often preferred for its compatibility with cuDNN implementations.\\n\\nExample code:\\n```python\\nimport keras\\nfrom keras.layers import GRU\\n\\ngru = GRU(4, reset_after=True)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do the dropout and recurrent_dropout arguments affect the GRU layer?\",\n",
        "        \"output\": \"The `dropout` argument specifies the fraction of the units to drop during the linear transformation of the inputs, while `recurrent_dropout` specifies the fraction of the units to drop during the linear transformation of the recurrent state. These arguments help in regularizing the model to prevent overfitting by randomly dropping units during training.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import GRU\\n\\ninputs = np.random.random((32, 10, 8))\\ngru = GRU(4, dropout=0.2, recurrent_dropout=0.2)\\noutput = gru(inputs)\\nprint(output.shape)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the stateful argument do in the GRU layer?\",\n",
        "        \"output\": \"The `stateful` argument, when set to True, allows the GRU layer to maintain the state between batches. This means that the hidden state from one batch will be used as the initial state for the next batch. This is useful for tasks where sequences are split across multiple batches but should be treated as a continuous sequence.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import GRU\\n\\ninputs = np.random.random((32, 10, 8))\\ngru = GRU(4, stateful=True)\\noutput = gru(inputs)\\nprint(output.shape)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the main purpose of the SimpleRNN layer in a neural network?\",\n",
        "        \"output\": \"The SimpleRNN layer is used to capture temporal dependencies in sequential data. It is a type of recurrent neural network that processes sequences by maintaining a hidden state that gets updated at each time step based on the input and the previous hidden state. This allows the network to learn and model relationships over time in sequential data.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import SimpleRNN\\n\\ninputs = np.random.random((32, 10, 8))\\nsimple_rnn = SimpleRNN(4)\\noutput = simple_rnn(inputs)\\nprint(output.shape)\\n# Output: (32, 4)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does the return_sequences argument affect the output of the SimpleRNN layer?\",\n",
        "        \"output\": \"The `return_sequences` argument specifies whether the SimpleRNN layer returns the output for each time step in the input sequence or only the output from the last time step. When `return_sequences` is set to True, the output will be a 3D tensor with shape (batch_size, timesteps, units). When set to False, the output will be a 2D tensor with shape (batch_size, units).\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import SimpleRNN\\n\\ninputs = np.random.random((32, 10, 8))\\nsimple_rnn = SimpleRNN(4, return_sequences=True)\\noutput = simple_rnn(inputs)\\nprint(output.shape)\\n# Output: (32, 10, 4)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the stateful argument do in the SimpleRNN layer?\",\n",
        "        \"output\": \"The `stateful` argument allows the SimpleRNN layer to maintain its state across batches. When `stateful` is set to True, the hidden state from the last sample in one batch is used as the initial state for the first sample in the next batch. This can be useful for tasks where sequences are split across batches but should be processed as a continuous sequence.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import SimpleRNN\\n\\ninputs = np.random.random((32, 10, 8))\\nsimple_rnn = SimpleRNN(4, stateful=True)\\noutput = simple_rnn(inputs)\\nprint(output.shape)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do dropout and recurrent_dropout arguments affect the SimpleRNN layer?\",\n",
        "        \"output\": \"The `dropout` argument specifies the fraction of the units to drop during the linear transformation of the inputs, while `recurrent_dropout` specifies the fraction of the units to drop during the linear transformation of the recurrent state. These arguments help regularize the model and prevent overfitting by randomly dropping units during training.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import SimpleRNN\\n\\ninputs = np.random.random((32, 10, 8))\\nsimple_rnn = SimpleRNN(4, dropout=0.2, recurrent_dropout=0.2)\\noutput = simple_rnn(inputs)\\nprint(output.shape)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the unroll argument do in the SimpleRNN layer?\",\n",
        "        \"output\": \"The `unroll` argument specifies whether the network should be unrolled or not. If `unroll` is set to True, the RNN will be unrolled, meaning that each time step is represented explicitly in the computation graph. This can speed up training for short sequences but is more memory-intensive. If `unroll` is False, a symbolic loop is used, which is more suitable for longer sequences.\\n\\nExample code:\\n```python\\nimport numpy as np\\nimport keras\\nfrom keras.layers import SimpleRNN\\n\\ninputs = np.random.random((32, 10, 8))\\nsimple_rnn = SimpleRNN(4, unroll=True)\\noutput = simple_rnn(inputs)\\nprint(output.shape)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the Bidirectional layer in a neural network?\",\n",
        "        \"output\": \"The `Bidirectional` layer is used to process sequences in both forward and backward directions, allowing the network to have access to context from both directions. This can improve the model's understanding of temporal dependencies in the data by combining information from the past and the future. The outputs from the forward and backward passes are combined according to the specified `merge_mode`.\\n\\nExample code:\\n```python\\nfrom keras.layers import Bidirectional, LSTM, Dense, Input\\nfrom keras.models import Sequential\\n\\nmodel = Sequential([\\n    Input(shape=(5, 10)),\\n    Bidirectional(LSTM(10, return_sequences=True)),\\n    Bidirectional(LSTM(10)),\\n    Dense(5, activation=\\\"softmax\\\"),\\n])\\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does the merge_mode argument affect the output of the Bidirectional layer?\",\n",
        "        \"output\": '''The `merge_mode` argument determines how the outputs from the forward and backward RNNs are combined. The options are:\\n- `\"sum\"`: Element-wise sum of the forward and backward outputs.\\n- `\"mul\"`: Element-wise product of the forward and backward outputs.\\n- `\"concat\"`: Concatenation of the forward and backward outputs along the last axis.\\n- `\"ave\"`: Element-wise average of the forward and backward outputs.\\n- `None`: Outputs are returned as a list of two separate tensors.\\n\\nExample code:\\n```python\\nfrom keras.layers import Bidirectional, LSTM, Dense, Input\\nfrom keras.models import Sequential\\n\\nmodel = Sequential([\\n    Input(shape=(5, 10)),\\n    Bidirectional(LSTM(10, return_sequences=True), merge_mode='concat'),\\n    Dense(5, activation=\\\"softmax\\\"),\\n])\\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\\n'''\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the role of the backward_layer argument in the Bidirectional layer?\",\n",
        "        \"output\": \"The `backward_layer` argument specifies an optional RNN layer or a Layer instance that processes the input sequence in the reverse direction. If provided, this custom backward layer is used in place of automatically generated backward RNNs. The `backward_layer` must have properties matching those of the `layer` argument, such as `go_backwards`, `return_sequences`, and `return_state`, but with the `go_backwards` property set to True.\\n\\nExample code:\\n```python\\nfrom keras.layers import Bidirectional, LSTM, Dense, Input\\nfrom keras.models import Sequential\\n\\nforward_layer = LSTM(10, return_sequences=True)\\nbackward_layer = LSTM(10, activation='relu', return_sequences=True, go_backwards=True)\\n\\nmodel = Sequential([\\n    Input(shape=(5, 10)),\\n    Bidirectional(forward_layer, backward_layer=backward_layer),\\n    Dense(5, activation=\\\"softmax\\\"),\\n])\\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do the call arguments for the Bidirectional layer work with respect to initial_state?\",\n",
        "        \"output\": \"When passing the `initial_state` argument to the Bidirectional layer, the first half of the `initial_state` list is used for the forward RNN, and the second half is used for the backward RNN. This allows you to initialize the hidden states of both the forward and backward RNNs independently.\\n\\nExample code:\\n```python\\nfrom keras.layers import Bidirectional, LSTM, Dense, Input\\nfrom keras.models import Sequential\\n\\ninputs = Input(shape=(5, 10))\\nforward_layer = LSTM(10, return_sequences=True)\\nbackward_layer = LSTM(10, return_sequences=True, go_backwards=True)\\n\\nbidirectional_layer = Bidirectional(forward_layer, backward_layer=backward_layer)\\noutputs = bidirectional_layer(inputs)\\n\\nmodel = Sequential([\\n    inputs,\\n    bidirectional_layer,\\n    Dense(5, activation=\\\"softmax\\\"),\\n])\\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What are the requirements for the layer argument in the Bidirectional layer?\",\n",
        "        \"output\": \"The `layer` argument in the Bidirectional layer should be an instance of an RNN layer, such as `LSTM` or `GRU`, or a Layer instance that meets the following criteria:\\n- It should be a sequence-processing layer that accepts 3D+ inputs.\\n- It should have the attributes `go_backwards`, `return_sequences`, and `return_state` with the same semantics as in RNN layers.\\n- It should have an `input_spec` attribute.\\n- It should implement serialization via `get_config()` and `from_config()`.\\n\\nExample code:\\n```python\\nfrom keras.layers import Bidirectional, LSTM, Dense, Input\\nfrom keras.models import Sequential\\n\\nmodel = Sequential([\\n    Input(shape=(5, 10)),\\n    Bidirectional(LSTM(10, return_sequences=True)),\\n    Dense(5, activation=\\\"softmax\\\"),\\n])\\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the `image_dataset_from_directory` function do in Keras?\",\n",
        "        \"output\": \"The `image_dataset_from_directory` function generates a `tf.data.Dataset` object from image files stored in a directory. It allows for loading, preprocessing, and batching images and their associated labels (if available) from a directory structure where subdirectories represent different classes. This function can handle image resizing, shuffling, and splitting into training and validation sets if specified.\\n\\nExample code:\\n```python\\nfrom keras.utils import image_dataset_from_directory\\n\\ndataset = image_dataset_from_directory(\\n    'path/to/directory',\\n    labels='inferred',\\n    batch_size=32,\\n    image_size=(256, 256)\\n)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you specify the size of images when using `image_dataset_from_directory`?\",\n",
        "        \"output\": \"The size of images is specified using the `image_size` argument in the `image_dataset_from_directory` function. This argument should be a tuple representing the target height and width of the images (e.g., `(256, 256)`). This ensures that all images are resized to the specified dimensions during loading.\\n\\nExample code:\\n```python\\ndataset = image_dataset_from_directory(\\n    'path/to/directory',\\n    image_size=(256, 256)\\n)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What options are available for the `label_mode` argument in `image_dataset_from_directory`?\",\n",
        "        \"output\": '''The `label_mode` argument determines the format of the labels in the dataset. Options include:\\n- `\"int\"`: Labels are encoded as integers.\\n- `\"categorical\"`: Labels are encoded as one-hot vectors.\\n- `\"binary\"`: Labels are encoded as float32 scalars with values 0 or 1.\\n- `None`: No labels are included.\\n\\nExample code:\\n```python\\ndataset = image_dataset_from_directory(\\n    'path/to/directory',\\n    label_mode='categorical'\\n)\\n'''\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the `color_mode` argument control in the `image_dataset_from_directory` function?\",\n",
        "        \"output\": '''The `color_mode` argument specifies the color channels of the images. Options include:\\n- `\"grayscale\"`: Load images in grayscale with 1 channel.\\n- `\"rgb\"`: Load images in RGB color mode with 3 channels.\\n- `\"rgba\"`: Load images in RGBA color mode with 4 channels.\\n\\nExample code:\\n```python\\ndataset = image_dataset_from_directory(\\n    'path/to/directory',\\n    color_mode='grayscale'\\n)\\n'''\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you use the `validation_split` and `subset` arguments in `image_dataset_from_directory`?\",\n",
        "        \"output\": '''\"The `validation_split` argument is used to reserve a fraction of the data for validation. It should be a float between 0 and 1. The `subset` argument determines which subset of data to return: `\"training\"` for training data, `\"validation\"` for validation data, or `\"both\"` for both. The `subset` argument is only used if `validation_split` is specified.\\n\\nExample code:\\n```python\\ntrain_dataset = image_dataset_from_directory(\\n    'path/to/directory',\\n    validation_split=0.2,\\n    subset='training'\\n)\\nvalidation_dataset = image_dataset_from_directory(\\n    'path/to/directory',\\n    validation_split=0.2,\\n    subset='validation'\\n)\\n```\"'''\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the `interpolation` argument in `image_dataset_from_directory`?\",\n",
        "        \"output\": '''The `interpolation` argument specifies the method used to resize images if they do not match the target size. Supported methods include `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`, `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, and `\"mitchellcubic\"`. The default is `\"bilinear\"`. This affects how image pixels are resampled during resizing.\\n\\nExample code:\\n```python\\ndataset = image_dataset_from_directory(\\n    'path/to/directory',\\n    interpolation='nearest'\\n)\\n'''\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you load an image and convert it to a NumPy array using Keras utilities?\",\n",
        "        \"output\": \"You can use the `load_img` function to load an image into a PIL Image instance and then use `img_to_array` to convert it to a NumPy array.\\n\\nExample code:\\n```python\\nfrom keras.utils import load_img, img_to_array\\n\\nimage = load_img('path/to/image.jpg')\\ninput_arr = img_to_array(image)\\ninput_arr = np.array([input_arr])  # Convert single image to a batch.\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the `save_img` function do?\",\n",
        "        \"output\": \"The `save_img` function saves a NumPy array as an image file. You can specify the path or file object where the image should be saved, the data format (channels_first or channels_last), and whether to rescale image values to the range [0, 255].\\n\\nExample code:\\n```python\\nfrom keras.utils import save_img\\n\\nsave_img('path/to/save/image.jpg', x=input_arr)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the function of the `array_to_img` function in Keras?\",\n",
        "        \"output\": \"The `array_to_img` function converts a 3D NumPy array to a PIL Image instance. This is useful for visualizing or saving images that are stored as NumPy arrays.\\n\\nExample code:\\n```python\\nfrom keras.utils import array_to_img\\n\\nimg_array = np.random.random((100, 100, 3))\\nimg = array_to_img(img_array)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the `keep_aspect_ratio` argument do in the `load_img` function?\",\n",
        "        \"output\": \"The `keep_aspect_ratio` argument determines whether the image should be resized to the target size while preserving its aspect ratio. If set to True, the image is resized without distortion, cropping the center if necessary. If False, the image may be resized with distortion to fit the target size.\\n\\nExample code:\\n```python\\nimage = load_img('path/to/image.jpg', target_size=(256, 256), keep_aspect_ratio=True)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you convert an image to a NumPy array and then back to an image using Keras utilities?\",\n",
        "        \"output\": \"You can convert an image to a NumPy array using `img_to_array` and then convert it back to a PIL Image instance using `array_to_img`.\\n\\nExample code:\\n```python\\nfrom keras.utils import load_img, img_to_array, array_to_img\\n\\nimage = load_img('path/to/image.jpg')\\narray = img_to_array(image)\\nnew_image = array_to_img(array)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you instantiate a VGG16 model with pre-trained weights from ImageNet and include the fully connected layers?\",\n",
        "        \"output\": \"You can instantiate the VGG16 model with pre-trained weights and include the fully connected layers using the following code:\\n\\n```python\\nfrom keras.applications import VGG16\\nmodel = VGG16(weights='imagenet', include_top=True)\\n```\\n\\nThis code initializes the VGG16 model with weights pre-trained on the ImageNet dataset and includes the top fully-connected layers, which are typically used for classification tasks.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the 'input_shape' argument in VGG16, and how would you use it if 'include_top' is set to False?\",\n",
        "        \"output\": \"The 'input_shape' argument specifies the shape of the input images to the model. When 'include_top' is set to False, you need to provide this argument. For example, if you want to use an input shape of 150x150 pixels with 3 color channels, you would use:\\n\\n```python\\nmodel = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\\n```\\n\\nThis code sets up the model for feature extraction, allowing you to process images of the specified size without including the fully-connected classification layers.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you use the VGG19 model for feature extraction, excluding the top layers, and what is the purpose of global average pooling in this context?\",\n",
        "        \"output\": \"To use VGG19 for feature extraction without the top layers, set 'include_top' to False and apply global average pooling to get a fixed-size output. Heres the code:\\n\\n```python\\nfrom keras.applications import VGG19\\nfrom keras.layers import GlobalAveragePooling2D\\nfrom keras.models import Model\\n\\nbase_model = VGG19(weights='imagenet', include_top=False)\\noutput = base_model.output\\noutput = GlobalAveragePooling2D()(output)\\nmodel = Model(inputs=base_model.input, outputs=output)\\n```\\n\\nGlobal average pooling reduces each feature map to a single number by averaging all the values, resulting in a fixed-size vector that can be used for further processing or classification.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What preprocessing function should be used for inputs when using VGG16, and how does it prepare the data?\",\n",
        "        \"output\": \"The preprocessing function for VGG16 is `keras.applications.vgg16.preprocess_input`. It converts input images from RGB to BGR and zero-centers each color channel with respect to the ImageNet dataset. Heres how you use it:\\n\\n```python\\nfrom keras.applications.vgg16 import preprocess_input\\nimport numpy as np\\n\\n# Example image array\\nimage_array = np.random.random((224, 224, 3))\\npreprocessed_image = preprocess_input(image_array)\\n```\\n\\nThis prepares the image data for the model by ensuring that it is in the format expected by VGG16.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"If you want to use VGG16 with grayscale images, what should the 'color_mode' be set to, and what changes does it imply for the input shape?\",\n",
        "        \"output\": \"Set the 'color_mode' to 'grayscale' when using VGG16 with grayscale images. The 'input_shape' should be adjusted to have only one channel. Heres an example:\\n\\n```python\\nfrom keras.applications import VGG16\\n\\nmodel = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 1), color_mode='grayscale')\\n```\\n\\nThis changes the input shape to (224, 224, 1), indicating that each image has only one channel.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How do you specify the number of output classes in VGG19, and what should be done if you want to use a custom activation function on the top layer?\",\n",
        "        \"output\": \"To specify the number of output classes in VGG19, set the 'classes' argument when 'include_top' is True. To use a custom activation function, set 'classifier_activation' to the desired function. For example:\\n\\n```python\\nfrom keras.applications import VGG19\\n\\nmodel = VGG19(weights='imagenet', include_top=True, classes=10, classifier_activation='sigmoid')\\n```\\n\\nThis sets up VGG19 for a classification task with 10 classes and uses a sigmoid activation function on the top layer.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the 'pooling' argument do in VGG16 when 'include_top' is False, and what are the possible values for this argument?\",\n",
        "        \"output\": \"The 'pooling' argument specifies the pooling operation to apply after the last convolutional block when 'include_top' is False. Possible values are:\\n- 'None': No pooling, the output is a 4D tensor.\\n- 'avg': Global average pooling applied to reduce the tensor to a 2D tensor.\\n- 'max': Global max pooling applied to reduce the tensor to a 2D tensor.\\n\\nExample code for global average pooling:\\n\\n```python\\nfrom keras.applications import VGG16\\nfrom keras.layers import GlobalAveragePooling2D\\nfrom keras.models import Model\\n\\nbase_model = VGG16(weights='imagenet', include_top=False)\\noutput = GlobalAveragePooling2D()(base_model.output)\\nmodel = Model(inputs=base_model.input, outputs=output)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you load a specific image and preprocess it for VGG19 using Keras utilities?\",\n",
        "        \"output\": \"First, load the image using `keras.utils.load_img`, then convert it to a NumPy array and preprocess it with `keras.applications.vgg19.preprocess_input`. Example:\\n\\n```python\\nfrom keras.utils import load_img, img_to_array\\nfrom keras.applications.vgg19 import preprocess_input\\n\\n# Load image\\nimage = load_img('path_to_image.jpg', target_size=(224, 224))\\n\\n# Convert to array and preprocess\\nimage_array = img_to_array(image)\\nimage_array = preprocess_input(image_array)\\n\\n# Add batch dimension\\nimage_array = np.expand_dims(image_array, axis=0)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the 'verbose' argument control in the `image_dataset_from_directory` function, and how would you set it to display detailed information?\",\n",
        "        \"output\": \"The 'verbose' argument controls the amount of information displayed about the dataset. Setting it to 'True' will display detailed information. Example:\\n\\n```python\\nfrom keras.utils import image_dataset_from_directory\\n\\ndataset = image_dataset_from_directory(\\n    'path_to_directory',\\n    verbose=True\\n)\\n```\\n\\nThis will print details about the number of classes and the number of images in each class.\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How can you save an image stored as a NumPy array to a file using Keras utilities, and what arguments can be specified?\",\n",
        "        \"output\": \"Use `keras.utils.save_img` to save an image. You can specify the path, data format, file format, and whether to scale pixel values. Example:\\n\\n```python\\nfrom keras.utils import save_img\\nimport numpy as np\\n\\n# Example image array\\nimage_array = np.random.random((224, 224, 3))\\n\\n# Save image\\nsave_img('path_to_save_image.jpg', image_array, scale=True)\\n```\\n\\nThis saves the image to 'path_to_save_image.jpg', rescaling pixel values to the range [0, 255].\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the primary difference between ResNet50 and ResNet101?\",\n",
        "        \"output\": \"ResNet50 has 50 layers, while ResNet101 has 101 layers. The additional layers in ResNet101 allow it to learn more complex features, potentially improving performance on certain tasks. However, it also requires more computational resources and memory.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications import ResNet50, ResNet101\\n\\n# ResNet50\\nmodel50 = ResNet50(include_top=True, weights='imagenet')\\n\\n# ResNet101\\nmodel101 = ResNet101(include_top=True, weights='imagenet')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does ResNet152 differ from ResNet50 and ResNet101?\",\n",
        "        \"output\": \"ResNet152 has 152 layers, making it deeper than both ResNet50 and ResNet101. This increased depth can help in capturing more intricate patterns in the data, but it also increases computational requirements. Deeper networks may offer better performance for complex tasks but require more training time and resources.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications import ResNet152\\n\\n# ResNet152\\nmodel152 = ResNet152(include_top=True, weights='imagenet')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the purpose of the 'include_top' argument in ResNet models?\",\n",
        "        \"output\": \"'include_top' specifies whether to include the fully-connected (dense) layer at the top of the network. If 'include_top' is set to True, the model will include this layer, which is useful for classification tasks. If set to False, the model will only output features and is typically used for feature extraction.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications import ResNet50\\n\\n# Include top layer\\nmodel_with_top = ResNet50(include_top=True)\\n\\n# Exclude top layer\\nmodel_without_top = ResNet50(include_top=False)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"Why is 'weights' important when initializing a ResNet model?\",\n",
        "        \"output\": \"The 'weights' argument determines whether the model is initialized with pre-trained weights or random weights. Using 'imagenet' loads weights pre-trained on the ImageNet dataset, which can improve performance on similar tasks. If set to None, the model will be initialized randomly, which is typically used for training from scratch.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications import ResNet50\\n\\n# Load pre-trained weights\\nmodel_with_weights = ResNet50(weights='imagenet')\\n\\n# Initialize randomly\\nmodel_random = ResNet50(weights=None)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does the preprocessing function for ResNet differ from other models?\",\n",
        "        \"output\": \"ResNet models require specific preprocessing to convert images from RGB to BGR and zero-center each color channel with respect to the ImageNet dataset. This is different from some other models that may require different preprocessing steps.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications.resnet import preprocess_input\\nimport numpy as np\\n\\n# Example image\\nimage = np.random.random((224, 224, 3))\\n\\n# Preprocess the image for ResNet\\npreprocessed_image = preprocess_input(image)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What does the 'pooling' argument control in ResNet models?\",\n",
        "        \"output\": \"'pooling' controls the pooling mode used for feature extraction when 'include_top' is False. Options include:\\n- None: Outputs the 4D tensor of the last convolutional block.\\n- 'avg': Applies global average pooling, resulting in a 2D tensor.\\n- 'max': Applies global max pooling, also resulting in a 2D tensor.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications import ResNet50\\n\\n# No pooling\\nmodel_no_pooling = ResNet50(include_top=False, pooling=None)\\n\\n# Global average pooling\\nmodel_avg_pooling = ResNet50(include_top=False, pooling='avg')\\n\\n# Global max pooling\\nmodel_max_pooling = ResNet50(include_top=False, pooling='max')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the effect of changing the 'classes' argument in ResNet models?\",\n",
        "        \"output\": \"The 'classes' argument specifies the number of classes for the classification task. It is used only if 'include_top' is True. Changing this value adjusts the output layer of the model to match the number of classes in your dataset.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications import ResNet50\\n\\n# Model for 10 classes\\nmodel_10_classes = ResNet50(include_top=True, classes=10)\\n\\n# Model for 100 classes\\nmodel_100_classes = ResNet50(include_top=True, classes=100)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"When should you use ResNet50V2 instead of ResNet50?\",\n",
        "        \"output\": \"ResNet50V2 includes improvements such as identity mappings that help with gradient flow and improve training performance. It can be preferable to use ResNet50V2 if you need better performance or training efficiency compared to ResNet50.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications import ResNet50V2\\n\\n# ResNet50V2\\nmodel_v2 = ResNet50V2(include_top=True, weights='imagenet')\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"How does the preprocessing function differ between ResNet and ResNetV2?\",\n",
        "        \"output\": \"ResNet uses `keras.applications.resnet.preprocess_input`, which converts images from RGB to BGR and zero-centers each channel. ResNetV2 uses `keras.applications.resnet_v2.preprocess_input`, which scales input pixels between -1 and 1.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications.resnet import preprocess_input as preprocess_resnet\\nfrom tensorflow.keras.applications.resnet_v2 import preprocess_input as preprocess_resnetv2\\nimport numpy as np\\n\\n# Example image\\nimage = np.random.random((224, 224, 3))\\n\\n# Preprocess for ResNet\\npreprocessed_resnet = preprocess_resnet(image)\\n\\n# Preprocess for ResNetV2\\npreprocessed_resnetv2 = preprocess_resnetv2(image)\\n```\"\n",
        "    },\n",
        "    {\n",
        "        \"text_input\": \"What is the advantage of using deeper ResNet architectures like ResNet152V2 over shallower ones?\",\n",
        "        \"output\": \"Deeper ResNet architectures like ResNet152V2 can capture more complex patterns and features in the data due to their increased depth. This can potentially lead to better performance on challenging tasks but also requires more computational resources and longer training times. For simpler tasks or when computational resources are limited, shallower architectures may be more appropriate.\\n\\nCode example:\\n```python\\nfrom tensorflow.keras.applications import ResNet152V2\\n\\n# ResNet152V2\\nmodel_152v2 = ResNet152V2(include_top=True, weights='imagenet')\\n```\"\n",
        "    }\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "y0itxNVF33Ge",
        "outputId": "7bd96b1e-1e84-41f6-e98c-33fe339f2cf9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 114,\n  \"fields\": [\n    {\n      \"column\": \"text_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 113,\n        \"samples\": [\n          \"How do the call arguments for the Bidirectional layer work with respect to initial_state?\",\n          \"How do you create a custom model by subclassing the Model class in Keras?\",\n          \"How is the leaky ReLU activation function different from ReLU?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 114,\n        \"samples\": [\n          \"The `backward_layer` argument specifies an optional RNN layer or a Layer instance that processes the input sequence in the reverse direction. If provided, this custom backward layer is used in place of automatically generated backward RNNs. The `backward_layer` must have properties matching those of the `layer` argument, such as `go_backwards`, `return_sequences`, and `return_state`, but with the `go_backwards` property set to True.\\n\\nExample code:\\n```python\\nfrom keras.layers import Bidirectional, LSTM, Dense, Input\\nfrom keras.models import Sequential\\n\\nforward_layer = LSTM(10, return_sequences=True)\\nbackward_layer = LSTM(10, activation='relu', return_sequences=True, go_backwards=True)\\n\\nmodel = Sequential([\\n    Input(shape=(5, 10)),\\n    Bidirectional(forward_layer, backward_layer=backward_layer),\\n    Dense(5, activation=\\\"softmax\\\"),\\n])\\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\\n```\",\n          \"To create a custom model by subclassing the Model class in Keras, you define the layers in the `__init__()` method and implement the forward pass in the `call()` method. For example:\\n\\n```python\\nclass MyModel(keras.Model):\\n    def __init__(self):\\n        super().__init__()\\n        self.dense1 = keras.layers.Dense(32, activation=\\\"relu\\\")\\n        self.dense2 = keras.layers.Dense(5, activation=\\\"softmax\\\")\\n\\n    def call(self, inputs):\\n        x = self.dense1(inputs)\\n        return self.dense2(x)\\n\\nmodel = MyModel()\\n```\",\n          \"The ELU (Exponential Linear Unit) activation function is defined as:\\n\\n```python\\nx if x > 0\\nalpha * exp(x) - 1 if x < 0\\n```\\nELUs have negative values which push the mean of activations closer to zero, helping with faster learning. For example:\\n\\n```python\\nimport keras\\nx = [-1, 0, 1]\\nprint(keras.activations.elu(x, alpha=1.0))\\n```\\nELUs provide a smoother transition for negative inputs compared to ReLU.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-75b6294d-8863-4b11-945a-ef736fa0211b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the Model class in Keras?</td>\n",
              "      <td>The Model class in Keras is an object that gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How do you create a model using the Functional...</td>\n",
              "      <td>To create a model using the Functional API in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Can you use nested inputs when creating models...</td>\n",
              "      <td>No, nested inputs are not supported in the Fun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How can you create sub-models from a larger Fu...</td>\n",
              "      <td>You can create sub-models by using intermediat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do you create a custom model by subclassin...</td>\n",
              "      <td>To create a custom model by subclassing the Mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75b6294d-8863-4b11-945a-ef736fa0211b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-75b6294d-8863-4b11-945a-ef736fa0211b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-75b6294d-8863-4b11-945a-ef736fa0211b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-023f8055-2ac0-468d-8c4a-3428e46881d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-023f8055-2ac0-468d-8c4a-3428e46881d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-023f8055-2ac0-468d-8c4a-3428e46881d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                          text_input  \\\n",
              "0                  What is the Model class in Keras?   \n",
              "1  How do you create a model using the Functional...   \n",
              "2  Can you use nested inputs when creating models...   \n",
              "3  How can you create sub-models from a larger Fu...   \n",
              "4  How do you create a custom model by subclassin...   \n",
              "\n",
              "                                              output  \n",
              "0  The Model class in Keras is an object that gro...  \n",
              "1  To create a model using the Functional API in ...  \n",
              "2  No, nested inputs are not supported in the Fun...  \n",
              "3  You can create sub-models by using intermediat...  \n",
              "4  To create a custom model by subclassing the Mo...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#saving dataset\n",
        "df = pd.DataFrame(dataset,columns=['text_input','output'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT_f1My54EVA",
        "outputId": "daf76b0c-31e8-4f82-d599-8f413085ba87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(114, 2)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape#so around 114 queries used to tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd1IYSO9yGz9",
        "outputId": "a33d4772-1e4c-4364-b00c-130bdf86049a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Model(name='models/gemini-1.0-pro-001',\n",
              "      base_model_id='',\n",
              "      version='001',\n",
              "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                   'model that supports tuning.'),\n",
              "      input_token_limit=30720,\n",
              "      output_token_limit=2048,\n",
              "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "      temperature=0.9,\n",
              "      max_temperature=None,\n",
              "      top_p=1.0,\n",
              "      top_k=None)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model = [\n",
        "    m for m in genai.list_models()\n",
        "    if \"createTunedModel\" in m.supported_generation_methods][0]\n",
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949,
          "referenced_widgets": [
            "a7781125c7b6418292524071c36bd91a",
            "22dc53d04e8f4087a9a9b9ea47c7b309",
            "b98e13523b7f438daac143f1991e2455",
            "33d6d095901d455bbed74c429a3f33f6",
            "cd31641901ce4866824b4d88e09d83e9",
            "203912e07c5f4640aa0ba9e16192bece",
            "e58fa538fb434f8e9492526beb031b59",
            "4a0aac949f864bdd8f7b7d16681c2941",
            "886062f309834c9d969eeae3608a3d0f",
            "09b2b43f17ae462796d5d91b24b1d93f",
            "8e7187ce8eb34f70a34858af50f4cdaf"
          ]
        },
        "id": "JKwIGHUyx06a",
        "outputId": "5a3e3113-bdcd-4c91-d191-75b853dc6e01"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7781125c7b6418292524071c36bd91a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/285 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TunedModel(name='tunedModels/finetuninggemmafordl1-xxcubsl6ftaf',\n",
            "           source_model='models/gemini-1.0-pro-001',\n",
            "           base_model='models/gemini-1.0-pro-001',\n",
            "           display_name='fineTuning-gemmafordl-1',\n",
            "           description='',\n",
            "           temperature=0.3,\n",
            "           top_p=1.0,\n",
            "           top_k=0,\n",
            "           state=<State.ACTIVE: 2>,\n",
            "           create_time=datetime.datetime(2024, 9, 15, 1, 54, 50, 823359, tzinfo=datetime.timezone.utc),\n",
            "           update_time=datetime.datetime(2024, 9, 15, 1, 58, 57, 63484, tzinfo=datetime.timezone.utc),\n",
            "           tuning_task=TuningTask(start_time=datetime.datetime(2024, 9, 15, 1, 54, 51, 90125, tzinfo=datetime.timezone.utc),\n",
            "                                  complete_time=datetime.datetime(2024, 9, 15, 1, 58, 57, 63484, tzinfo=datetime.timezone.utc),\n",
            "                                  snapshots=[...],\n",
            "                                  hyperparameters=Hyperparameters(epoch_count=10,\n",
            "                                                                  batch_size=4,\n",
            "                                                                  learning_rate=0.001)))\n",
            "```python\n",
            "from keras.layers import Input, Dense\n",
            "from keras.models import Model\n",
            "\n",
            "inputs = Input(shape=(7,))\n",
            "x = Dense(32, activation=\"relu\")(inputs)\n",
            "x = Dense(16, activation=\"relu\")(x)\n",
            "outputs = Dense(5, activation=\"softmax\")(x)\n",
            "model = Model(inputs=inputs, outputs=outputs)\n",
            "```\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#using the gemini-1.0-pro-001\n",
        "operation = genai.create_tuned_model(\n",
        "    # You can use a tuned model here too. Set `source_model=\"tunedModels/...\"`\n",
        "    display_name=\"fineTuning-gemmafordl-1\",\n",
        "    source_model=base_model.name,\n",
        "    epoch_count=10,\n",
        "    batch_size=4,\n",
        "    learning_rate=0.001,\n",
        "    training_data=dataset,\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "for status in operation.wait_bar():\n",
        "    time.sleep(10)\n",
        "result = operation.result()\n",
        "print(result)\n",
        "# # You can plot the loss curve with:\n",
        "snapshots = pd.DataFrame(result.tuning_task.snapshots)\n",
        "sns.lineplot(data=snapshots, x='epoch', y='mean_loss')\n",
        "\n",
        "model = genai.GenerativeModel(model_name=result.name)\n",
        "result = model.generate_content(\"How do you create a model using the Functional API in Keras?\")\n",
        "print(result.text)  # shd give proper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "XXAvbpF5aYUb",
        "outputId": "bf6e419f-52ed-446d-ee62-367ac49046e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"snapshots\",\n  \"rows\": 285,\n  \"fields\": [\n    {\n      \"column\": \"step\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82,\n        \"min\": 1,\n        \"max\": 285,\n        \"num_unique_values\": 285,\n        \"samples\": [\n          10,\n          250,\n          158\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29.140495211956917,\n        \"min\": 0.30540437,\n        \"max\": 147.60696,\n        \"num_unique_values\": 285,\n        \"samples\": [\n          113.66089,\n          1.1743846,\n          3.2119727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 10,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          5,\n          0,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"compute_time\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-09-15 01:54:53.422404+00:00\",\n        \"max\": \"2024-09-15 01:58:56.928332+00:00\",\n        \"num_unique_values\": 285,\n        \"samples\": [\n          \"2024-09-15 01:55:00.929617+00:00\",\n          \"2024-09-15 01:58:25.909488+00:00\",\n          \"2024-09-15 01:57:07.081173+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "snapshots"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-cf773f3d-adbf-4008-af76-71665b8aa6d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>mean_loss</th>\n",
              "      <th>epoch</th>\n",
              "      <th>compute_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>123.914170</td>\n",
              "      <td>0</td>\n",
              "      <td>2024-09-15 01:54:53.422404+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>115.627070</td>\n",
              "      <td>0</td>\n",
              "      <td>2024-09-15 01:54:54.250795+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>103.677635</td>\n",
              "      <td>0</td>\n",
              "      <td>2024-09-15 01:54:55.138575+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>147.606960</td>\n",
              "      <td>0</td>\n",
              "      <td>2024-09-15 01:54:55.873325+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>97.989690</td>\n",
              "      <td>0</td>\n",
              "      <td>2024-09-15 01:54:56.598029+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>281</td>\n",
              "      <td>6.068657</td>\n",
              "      <td>9</td>\n",
              "      <td>2024-09-15 01:58:52.372188+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>282</td>\n",
              "      <td>1.909381</td>\n",
              "      <td>9</td>\n",
              "      <td>2024-09-15 01:58:53.049770+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>283</td>\n",
              "      <td>0.583945</td>\n",
              "      <td>9</td>\n",
              "      <td>2024-09-15 01:58:53.802243+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>284</td>\n",
              "      <td>3.141478</td>\n",
              "      <td>9</td>\n",
              "      <td>2024-09-15 01:58:54.753273+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>285</td>\n",
              "      <td>2.210561</td>\n",
              "      <td>10</td>\n",
              "      <td>2024-09-15 01:58:56.928332+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>285 rows  4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf773f3d-adbf-4008-af76-71665b8aa6d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf773f3d-adbf-4008-af76-71665b8aa6d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf773f3d-adbf-4008-af76-71665b8aa6d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-201bf8f9-8dc1-4a7b-8445-b30b1c0ca6ed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-201bf8f9-8dc1-4a7b-8445-b30b1c0ca6ed')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-201bf8f9-8dc1-4a7b-8445-b30b1c0ca6ed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8abad9ff-31b0-492d-9353-7e5361e6b0f7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('snapshots')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8abad9ff-31b0-492d-9353-7e5361e6b0f7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('snapshots');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     step   mean_loss  epoch                     compute_time\n",
              "0       1  123.914170      0 2024-09-15 01:54:53.422404+00:00\n",
              "1       2  115.627070      0 2024-09-15 01:54:54.250795+00:00\n",
              "2       3  103.677635      0 2024-09-15 01:54:55.138575+00:00\n",
              "3       4  147.606960      0 2024-09-15 01:54:55.873325+00:00\n",
              "4       5   97.989690      0 2024-09-15 01:54:56.598029+00:00\n",
              "..    ...         ...    ...                              ...\n",
              "280   281    6.068657      9 2024-09-15 01:58:52.372188+00:00\n",
              "281   282    1.909381      9 2024-09-15 01:58:53.049770+00:00\n",
              "282   283    0.583945      9 2024-09-15 01:58:53.802243+00:00\n",
              "283   284    3.141478      9 2024-09-15 01:58:54.753273+00:00\n",
              "284   285    2.210561     10 2024-09-15 01:58:56.928332+00:00\n",
              "\n",
              "[285 rows x 4 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snapshots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt9Nf7HDbrTg",
        "outputId": "35c9de14-4c0f-4148-9ce3-99a8407aaf88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "total_steps: 285\n",
              "completed_steps: 285\n",
              "completed_percent: 100.0\n",
              "snapshots {\n",
              "  step: 1\n",
              "  mean_loss: 123.91416931152344\n",
              "  compute_time {\n",
              "    seconds: 1726365293\n",
              "    nanos: 422403831\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 2\n",
              "  mean_loss: 115.62706756591797\n",
              "  compute_time {\n",
              "    seconds: 1726365294\n",
              "    nanos: 250795237\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 3\n",
              "  mean_loss: 103.6776351928711\n",
              "  compute_time {\n",
              "    seconds: 1726365295\n",
              "    nanos: 138574846\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 4\n",
              "  mean_loss: 147.60696411132812\n",
              "  compute_time {\n",
              "    seconds: 1726365295\n",
              "    nanos: 873325078\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 5\n",
              "  mean_loss: 97.98969268798828\n",
              "  compute_time {\n",
              "    seconds: 1726365296\n",
              "    nanos: 598028524\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 6\n",
              "  mean_loss: 96.13219451904297\n",
              "  compute_time {\n",
              "    seconds: 1726365297\n",
              "    nanos: 381608284\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 7\n",
              "  mean_loss: 96.79936218261719\n",
              "  compute_time {\n",
              "    seconds: 1726365298\n",
              "    nanos: 136868641\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 8\n",
              "  mean_loss: 62.67908477783203\n",
              "  compute_time {\n",
              "    seconds: 1726365299\n",
              "    nanos: 3816540\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 9\n",
              "  mean_loss: 106.77759552001953\n",
              "  compute_time {\n",
              "    seconds: 1726365299\n",
              "    nanos: 987782720\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 10\n",
              "  mean_loss: 113.660888671875\n",
              "  compute_time {\n",
              "    seconds: 1726365300\n",
              "    nanos: 929617333\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 11\n",
              "  mean_loss: 99.55667114257812\n",
              "  compute_time {\n",
              "    seconds: 1726365301\n",
              "    nanos: 735184985\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 12\n",
              "  mean_loss: 83.46298217773438\n",
              "  compute_time {\n",
              "    seconds: 1726365302\n",
              "    nanos: 548636566\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 13\n",
              "  mean_loss: 64.78334045410156\n",
              "  compute_time {\n",
              "    seconds: 1726365303\n",
              "    nanos: 439853349\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 14\n",
              "  mean_loss: 87.42951202392578\n",
              "  compute_time {\n",
              "    seconds: 1726365304\n",
              "    nanos: 377953625\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 15\n",
              "  mean_loss: 81.83836364746094\n",
              "  compute_time {\n",
              "    seconds: 1726365305\n",
              "    nanos: 637043056\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 16\n",
              "  mean_loss: 90.57007598876953\n",
              "  compute_time {\n",
              "    seconds: 1726365306\n",
              "    nanos: 548543027\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 17\n",
              "  mean_loss: 74.76764678955078\n",
              "  compute_time {\n",
              "    seconds: 1726365307\n",
              "    nanos: 462431964\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 18\n",
              "  mean_loss: 63.67045593261719\n",
              "  compute_time {\n",
              "    seconds: 1726365308\n",
              "    nanos: 345221368\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 19\n",
              "  mean_loss: 56.14754867553711\n",
              "  compute_time {\n",
              "    seconds: 1726365309\n",
              "    nanos: 207748659\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 20\n",
              "  mean_loss: 76.14798736572266\n",
              "  compute_time {\n",
              "    seconds: 1726365310\n",
              "    nanos: 204464050\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 21\n",
              "  mean_loss: 60.2005729675293\n",
              "  compute_time {\n",
              "    seconds: 1726365311\n",
              "    nanos: 167946913\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 22\n",
              "  mean_loss: 109.38587951660156\n",
              "  compute_time {\n",
              "    seconds: 1726365311\n",
              "    nanos: 887537497\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 23\n",
              "  mean_loss: 85.04651641845703\n",
              "  compute_time {\n",
              "    seconds: 1726365312\n",
              "    nanos: 671777518\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 24\n",
              "  mean_loss: 70.74461364746094\n",
              "  compute_time {\n",
              "    seconds: 1726365313\n",
              "    nanos: 641525731\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 25\n",
              "  mean_loss: 82.1708984375\n",
              "  compute_time {\n",
              "    seconds: 1726365314\n",
              "    nanos: 620308724\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 26\n",
              "  mean_loss: 101.68110656738281\n",
              "  compute_time {\n",
              "    seconds: 1726365315\n",
              "    nanos: 274533353\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 27\n",
              "  mean_loss: 99.47343444824219\n",
              "  compute_time {\n",
              "    seconds: 1726365315\n",
              "    nanos: 974942067\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 28\n",
              "  mean_loss: 71.0648422241211\n",
              "  compute_time {\n",
              "    seconds: 1726365316\n",
              "    nanos: 678932061\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 29\n",
              "  epoch: 1\n",
              "  mean_loss: 58.66441345214844\n",
              "  compute_time {\n",
              "    seconds: 1726365317\n",
              "    nanos: 537217406\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 30\n",
              "  epoch: 1\n",
              "  mean_loss: 73.15718841552734\n",
              "  compute_time {\n",
              "    seconds: 1726365318\n",
              "    nanos: 385172445\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 31\n",
              "  epoch: 1\n",
              "  mean_loss: 50.662620544433594\n",
              "  compute_time {\n",
              "    seconds: 1726365319\n",
              "    nanos: 34115207\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 32\n",
              "  epoch: 1\n",
              "  mean_loss: 53.90970993041992\n",
              "  compute_time {\n",
              "    seconds: 1726365319\n",
              "    nanos: 999433196\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 33\n",
              "  epoch: 1\n",
              "  mean_loss: 65.15113830566406\n",
              "  compute_time {\n",
              "    seconds: 1726365320\n",
              "    nanos: 826851984\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 34\n",
              "  epoch: 1\n",
              "  mean_loss: 63.091552734375\n",
              "  compute_time {\n",
              "    seconds: 1726365321\n",
              "    nanos: 773053553\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 35\n",
              "  epoch: 1\n",
              "  mean_loss: 34.53722381591797\n",
              "  compute_time {\n",
              "    seconds: 1726365322\n",
              "    nanos: 514497469\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 36\n",
              "  epoch: 1\n",
              "  mean_loss: 70.52420806884766\n",
              "  compute_time {\n",
              "    seconds: 1726365323\n",
              "    nanos: 468175825\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 37\n",
              "  epoch: 1\n",
              "  mean_loss: 37.867576599121094\n",
              "  compute_time {\n",
              "    seconds: 1726365324\n",
              "    nanos: 182363673\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 38\n",
              "  epoch: 1\n",
              "  mean_loss: 44.419532775878906\n",
              "  compute_time {\n",
              "    seconds: 1726365324\n",
              "    nanos: 888726342\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 39\n",
              "  epoch: 1\n",
              "  mean_loss: 46.39860916137695\n",
              "  compute_time {\n",
              "    seconds: 1726365325\n",
              "    nanos: 598239126\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 40\n",
              "  epoch: 1\n",
              "  mean_loss: 50.65623474121094\n",
              "  compute_time {\n",
              "    seconds: 1726365326\n",
              "    nanos: 561775635\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 41\n",
              "  epoch: 1\n",
              "  mean_loss: 49.969871520996094\n",
              "  compute_time {\n",
              "    seconds: 1726365327\n",
              "    nanos: 380889540\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 42\n",
              "  epoch: 1\n",
              "  mean_loss: 71.98133087158203\n",
              "  compute_time {\n",
              "    seconds: 1726365328\n",
              "    nanos: 330676405\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 43\n",
              "  epoch: 1\n",
              "  mean_loss: 52.2473030090332\n",
              "  compute_time {\n",
              "    seconds: 1726365329\n",
              "    nanos: 242630947\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 44\n",
              "  epoch: 1\n",
              "  mean_loss: 39.32090759277344\n",
              "  compute_time {\n",
              "    seconds: 1726365330\n",
              "    nanos: 63058297\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 45\n",
              "  epoch: 1\n",
              "  mean_loss: 45.26170349121094\n",
              "  compute_time {\n",
              "    seconds: 1726365330\n",
              "    nanos: 733418023\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 46\n",
              "  epoch: 1\n",
              "  mean_loss: 60.653419494628906\n",
              "  compute_time {\n",
              "    seconds: 1726365331\n",
              "    nanos: 583240367\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 47\n",
              "  epoch: 1\n",
              "  mean_loss: 66.49964904785156\n",
              "  compute_time {\n",
              "    seconds: 1726365332\n",
              "    nanos: 513980352\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 48\n",
              "  epoch: 1\n",
              "  mean_loss: 56.67047119140625\n",
              "  compute_time {\n",
              "    seconds: 1726365333\n",
              "    nanos: 306459920\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 49\n",
              "  epoch: 1\n",
              "  mean_loss: 51.41734313964844\n",
              "  compute_time {\n",
              "    seconds: 1726365333\n",
              "    nanos: 991310418\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 50\n",
              "  epoch: 1\n",
              "  mean_loss: 41.071964263916016\n",
              "  compute_time {\n",
              "    seconds: 1726365334\n",
              "    nanos: 842257877\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 51\n",
              "  epoch: 1\n",
              "  mean_loss: 32.423858642578125\n",
              "  compute_time {\n",
              "    seconds: 1726365335\n",
              "    nanos: 748756358\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 52\n",
              "  epoch: 1\n",
              "  mean_loss: 67.50741577148438\n",
              "  compute_time {\n",
              "    seconds: 1726365336\n",
              "    nanos: 617000759\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 53\n",
              "  epoch: 1\n",
              "  mean_loss: 63.12823486328125\n",
              "  compute_time {\n",
              "    seconds: 1726365337\n",
              "    nanos: 473811769\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 54\n",
              "  epoch: 1\n",
              "  mean_loss: 64.03775024414062\n",
              "  compute_time {\n",
              "    seconds: 1726365338\n",
              "    nanos: 124352654\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 55\n",
              "  epoch: 1\n",
              "  mean_loss: 48.70896530151367\n",
              "  compute_time {\n",
              "    seconds: 1726365339\n",
              "    nanos: 23005780\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 56\n",
              "  epoch: 1\n",
              "  mean_loss: 43.20685577392578\n",
              "  compute_time {\n",
              "    seconds: 1726365339\n",
              "    nanos: 927721306\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 57\n",
              "  epoch: 2\n",
              "  mean_loss: 77.66278076171875\n",
              "  compute_time {\n",
              "    seconds: 1726365340\n",
              "    nanos: 650941467\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 58\n",
              "  epoch: 2\n",
              "  mean_loss: 36.4754753112793\n",
              "  compute_time {\n",
              "    seconds: 1726365341\n",
              "    nanos: 566891497\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 59\n",
              "  epoch: 2\n",
              "  mean_loss: 46.26744842529297\n",
              "  compute_time {\n",
              "    seconds: 1726365342\n",
              "    nanos: 539605131\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 60\n",
              "  epoch: 2\n",
              "  mean_loss: 22.320514678955078\n",
              "  compute_time {\n",
              "    seconds: 1726365343\n",
              "    nanos: 393895450\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 61\n",
              "  epoch: 2\n",
              "  mean_loss: 18.001989364624023\n",
              "  compute_time {\n",
              "    seconds: 1726365344\n",
              "    nanos: 154376783\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 62\n",
              "  epoch: 2\n",
              "  mean_loss: 28.620222091674805\n",
              "  compute_time {\n",
              "    seconds: 1726365344\n",
              "    nanos: 867909254\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 63\n",
              "  epoch: 2\n",
              "  mean_loss: 43.92873764038086\n",
              "  compute_time {\n",
              "    seconds: 1726365345\n",
              "    nanos: 638872772\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 64\n",
              "  epoch: 2\n",
              "  mean_loss: 18.392547607421875\n",
              "  compute_time {\n",
              "    seconds: 1726365346\n",
              "    nanos: 288266714\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 65\n",
              "  epoch: 2\n",
              "  mean_loss: 38.17041015625\n",
              "  compute_time {\n",
              "    seconds: 1726365347\n",
              "    nanos: 245181941\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 66\n",
              "  epoch: 2\n",
              "  mean_loss: 21.015527725219727\n",
              "  compute_time {\n",
              "    seconds: 1726365348\n",
              "    nanos: 148605831\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 67\n",
              "  epoch: 2\n",
              "  mean_loss: 48.07582092285156\n",
              "  compute_time {\n",
              "    seconds: 1726365349\n",
              "    nanos: 47722260\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 68\n",
              "  epoch: 2\n",
              "  mean_loss: 30.57298469543457\n",
              "  compute_time {\n",
              "    seconds: 1726365349\n",
              "    nanos: 877370933\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 69\n",
              "  epoch: 2\n",
              "  mean_loss: 34.34423828125\n",
              "  compute_time {\n",
              "    seconds: 1726365350\n",
              "    nanos: 679930020\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 70\n",
              "  epoch: 2\n",
              "  mean_loss: 41.42940902709961\n",
              "  compute_time {\n",
              "    seconds: 1726365351\n",
              "    nanos: 411606749\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 71\n",
              "  epoch: 2\n",
              "  mean_loss: 19.08022689819336\n",
              "  compute_time {\n",
              "    seconds: 1726365352\n",
              "    nanos: 80770601\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 72\n",
              "  epoch: 2\n",
              "  mean_loss: 21.99800682067871\n",
              "  compute_time {\n",
              "    seconds: 1726365352\n",
              "    nanos: 808352484\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 73\n",
              "  epoch: 2\n",
              "  mean_loss: 29.04555892944336\n",
              "  compute_time {\n",
              "    seconds: 1726365354\n",
              "    nanos: 773916675\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 74\n",
              "  epoch: 2\n",
              "  mean_loss: 20.13228988647461\n",
              "  compute_time {\n",
              "    seconds: 1726365355\n",
              "    nanos: 760579375\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 75\n",
              "  epoch: 2\n",
              "  mean_loss: 29.418113708496094\n",
              "  compute_time {\n",
              "    seconds: 1726365356\n",
              "    nanos: 740169986\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 76\n",
              "  epoch: 2\n",
              "  mean_loss: 46.01127624511719\n",
              "  compute_time {\n",
              "    seconds: 1726365357\n",
              "    nanos: 428709505\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 77\n",
              "  epoch: 2\n",
              "  mean_loss: 40.1812629699707\n",
              "  compute_time {\n",
              "    seconds: 1726365358\n",
              "    nanos: 120415994\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 78\n",
              "  epoch: 2\n",
              "  mean_loss: 23.890830993652344\n",
              "  compute_time {\n",
              "    seconds: 1726365358\n",
              "    nanos: 831821752\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 79\n",
              "  epoch: 2\n",
              "  mean_loss: 29.084228515625\n",
              "  compute_time {\n",
              "    seconds: 1726365359\n",
              "    nanos: 699872662\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 80\n",
              "  epoch: 2\n",
              "  mean_loss: 52.46548080444336\n",
              "  compute_time {\n",
              "    seconds: 1726365360\n",
              "    nanos: 479164687\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 81\n",
              "  epoch: 2\n",
              "  mean_loss: 23.030941009521484\n",
              "  compute_time {\n",
              "    seconds: 1726365361\n",
              "    nanos: 441069139\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 82\n",
              "  epoch: 2\n",
              "  mean_loss: 25.547924041748047\n",
              "  compute_time {\n",
              "    seconds: 1726365362\n",
              "    nanos: 111423749\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 83\n",
              "  epoch: 2\n",
              "  mean_loss: 35.395015716552734\n",
              "  compute_time {\n",
              "    seconds: 1726365363\n",
              "    nanos: 126706820\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 84\n",
              "  epoch: 2\n",
              "  mean_loss: 57.40940475463867\n",
              "  compute_time {\n",
              "    seconds: 1726365364\n",
              "    nanos: 97846538\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 85\n",
              "  epoch: 2\n",
              "  mean_loss: 34.87519454956055\n",
              "  compute_time {\n",
              "    seconds: 1726365364\n",
              "    nanos: 912662409\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 86\n",
              "  epoch: 3\n",
              "  mean_loss: 24.509506225585938\n",
              "  compute_time {\n",
              "    seconds: 1726365365\n",
              "    nanos: 594542527\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 87\n",
              "  epoch: 3\n",
              "  mean_loss: 18.140424728393555\n",
              "  compute_time {\n",
              "    seconds: 1726365366\n",
              "    nanos: 243099074\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 88\n",
              "  epoch: 3\n",
              "  mean_loss: 17.159692764282227\n",
              "  compute_time {\n",
              "    seconds: 1726365367\n",
              "    nanos: 142042004\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 89\n",
              "  epoch: 3\n",
              "  mean_loss: 12.677687644958496\n",
              "  compute_time {\n",
              "    seconds: 1726365367\n",
              "    nanos: 833095022\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 90\n",
              "  epoch: 3\n",
              "  mean_loss: 20.938648223876953\n",
              "  compute_time {\n",
              "    seconds: 1726365368\n",
              "    nanos: 575404724\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 91\n",
              "  epoch: 3\n",
              "  mean_loss: 10.672837257385254\n",
              "  compute_time {\n",
              "    seconds: 1726365369\n",
              "    nanos: 396770327\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 92\n",
              "  epoch: 3\n",
              "  mean_loss: 7.988043308258057\n",
              "  compute_time {\n",
              "    seconds: 1726365370\n",
              "    nanos: 184621436\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 93\n",
              "  epoch: 3\n",
              "  mean_loss: 17.932636260986328\n",
              "  compute_time {\n",
              "    seconds: 1726365370\n",
              "    nanos: 961907535\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 94\n",
              "  epoch: 3\n",
              "  mean_loss: 19.90119171142578\n",
              "  compute_time {\n",
              "    seconds: 1726365371\n",
              "    nanos: 914358580\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 95\n",
              "  epoch: 3\n",
              "  mean_loss: 19.583646774291992\n",
              "  compute_time {\n",
              "    seconds: 1726365372\n",
              "    nanos: 788107004\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 96\n",
              "  epoch: 3\n",
              "  mean_loss: 9.121296882629395\n",
              "  compute_time {\n",
              "    seconds: 1726365373\n",
              "    nanos: 750294663\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 97\n",
              "  epoch: 3\n",
              "  mean_loss: 22.132476806640625\n",
              "  compute_time {\n",
              "    seconds: 1726365374\n",
              "    nanos: 487907767\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 98\n",
              "  epoch: 3\n",
              "  mean_loss: 16.47735595703125\n",
              "  compute_time {\n",
              "    seconds: 1726365375\n",
              "    nanos: 438116872\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 99\n",
              "  epoch: 3\n",
              "  mean_loss: 2.7744712829589844\n",
              "  compute_time {\n",
              "    seconds: 1726365376\n",
              "    nanos: 103971782\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 100\n",
              "  epoch: 3\n",
              "  mean_loss: 12.126398086547852\n",
              "  compute_time {\n",
              "    seconds: 1726365376\n",
              "    nanos: 828142063\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 101\n",
              "  epoch: 3\n",
              "  mean_loss: 16.5457820892334\n",
              "  compute_time {\n",
              "    seconds: 1726365377\n",
              "    nanos: 486268860\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 102\n",
              "  epoch: 3\n",
              "  mean_loss: 21.567245483398438\n",
              "  compute_time {\n",
              "    seconds: 1726365378\n",
              "    nanos: 168693000\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 103\n",
              "  epoch: 3\n",
              "  mean_loss: 25.57403564453125\n",
              "  compute_time {\n",
              "    seconds: 1726365379\n",
              "    nanos: 87852120\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 104\n",
              "  epoch: 3\n",
              "  mean_loss: 11.58110237121582\n",
              "  compute_time {\n",
              "    seconds: 1726365379\n",
              "    nanos: 873987883\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 105\n",
              "  epoch: 3\n",
              "  mean_loss: 7.438006401062012\n",
              "  compute_time {\n",
              "    seconds: 1726365380\n",
              "    nanos: 746145768\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 106\n",
              "  epoch: 3\n",
              "  mean_loss: 8.543132781982422\n",
              "  compute_time {\n",
              "    seconds: 1726365381\n",
              "    nanos: 701664681\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 107\n",
              "  epoch: 3\n",
              "  mean_loss: 13.013243675231934\n",
              "  compute_time {\n",
              "    seconds: 1726365382\n",
              "    nanos: 604122386\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 108\n",
              "  epoch: 3\n",
              "  mean_loss: 11.291495323181152\n",
              "  compute_time {\n",
              "    seconds: 1726365383\n",
              "    nanos: 533196415\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 109\n",
              "  epoch: 3\n",
              "  mean_loss: 19.625123977661133\n",
              "  compute_time {\n",
              "    seconds: 1726365384\n",
              "    nanos: 336873507\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 110\n",
              "  epoch: 3\n",
              "  mean_loss: 13.18955135345459\n",
              "  compute_time {\n",
              "    seconds: 1726365385\n",
              "    nanos: 254813072\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 111\n",
              "  epoch: 3\n",
              "  mean_loss: 31.326122283935547\n",
              "  compute_time {\n",
              "    seconds: 1726365385\n",
              "    nanos: 900691567\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 112\n",
              "  epoch: 3\n",
              "  mean_loss: 22.740617752075195\n",
              "  compute_time {\n",
              "    seconds: 1726365386\n",
              "    nanos: 627455913\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 113\n",
              "  epoch: 3\n",
              "  mean_loss: 28.907489776611328\n",
              "  compute_time {\n",
              "    seconds: 1726365387\n",
              "    nanos: 555325894\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 114\n",
              "  epoch: 4\n",
              "  mean_loss: 19.545522689819336\n",
              "  compute_time {\n",
              "    seconds: 1726365388\n",
              "    nanos: 272652266\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 115\n",
              "  epoch: 4\n",
              "  mean_loss: 12.46999454498291\n",
              "  compute_time {\n",
              "    seconds: 1726365389\n",
              "    nanos: 142773862\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 116\n",
              "  epoch: 4\n",
              "  mean_loss: 12.264358520507812\n",
              "  compute_time {\n",
              "    seconds: 1726365390\n",
              "    nanos: 9787049\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 117\n",
              "  epoch: 4\n",
              "  mean_loss: 6.5923662185668945\n",
              "  compute_time {\n",
              "    seconds: 1726365390\n",
              "    nanos: 985727728\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 118\n",
              "  epoch: 4\n",
              "  mean_loss: 4.777016639709473\n",
              "  compute_time {\n",
              "    seconds: 1726365391\n",
              "    nanos: 675217258\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 119\n",
              "  epoch: 4\n",
              "  mean_loss: 12.493806838989258\n",
              "  compute_time {\n",
              "    seconds: 1726365392\n",
              "    nanos: 370391037\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 120\n",
              "  epoch: 4\n",
              "  mean_loss: 7.028742790222168\n",
              "  compute_time {\n",
              "    seconds: 1726365393\n",
              "    nanos: 265561140\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 121\n",
              "  epoch: 4\n",
              "  mean_loss: 7.987512588500977\n",
              "  compute_time {\n",
              "    seconds: 1726365393\n",
              "    nanos: 952288053\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 122\n",
              "  epoch: 4\n",
              "  mean_loss: 25.696292877197266\n",
              "  compute_time {\n",
              "    seconds: 1726365394\n",
              "    nanos: 904867358\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 123\n",
              "  epoch: 4\n",
              "  mean_loss: 8.633781433105469\n",
              "  compute_time {\n",
              "    seconds: 1726365395\n",
              "    nanos: 758412875\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 124\n",
              "  epoch: 4\n",
              "  mean_loss: 8.97385311126709\n",
              "  compute_time {\n",
              "    seconds: 1726365396\n",
              "    nanos: 698007306\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 125\n",
              "  epoch: 4\n",
              "  mean_loss: 7.669810771942139\n",
              "  compute_time {\n",
              "    seconds: 1726365397\n",
              "    nanos: 538358260\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 126\n",
              "  epoch: 4\n",
              "  mean_loss: 7.224844455718994\n",
              "  compute_time {\n",
              "    seconds: 1726365398\n",
              "    nanos: 243765932\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 127\n",
              "  epoch: 4\n",
              "  mean_loss: 9.504402160644531\n",
              "  compute_time {\n",
              "    seconds: 1726365398\n",
              "    nanos: 920697463\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 128\n",
              "  epoch: 4\n",
              "  mean_loss: 13.821557998657227\n",
              "  compute_time {\n",
              "    seconds: 1726365399\n",
              "    nanos: 927107516\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 129\n",
              "  epoch: 4\n",
              "  mean_loss: 2.636523723602295\n",
              "  compute_time {\n",
              "    seconds: 1726365400\n",
              "    nanos: 766590728\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 130\n",
              "  epoch: 4\n",
              "  mean_loss: 7.288971900939941\n",
              "  compute_time {\n",
              "    seconds: 1726365401\n",
              "    nanos: 722183805\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 131\n",
              "  epoch: 4\n",
              "  mean_loss: 5.5844340324401855\n",
              "  compute_time {\n",
              "    seconds: 1726365402\n",
              "    nanos: 730539893\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 132\n",
              "  epoch: 4\n",
              "  mean_loss: 7.6761250495910645\n",
              "  compute_time {\n",
              "    seconds: 1726365403\n",
              "    nanos: 646673443\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 133\n",
              "  epoch: 4\n",
              "  mean_loss: 4.914300441741943\n",
              "  compute_time {\n",
              "    seconds: 1726365404\n",
              "    nanos: 335330080\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 134\n",
              "  epoch: 4\n",
              "  mean_loss: 9.1331787109375\n",
              "  compute_time {\n",
              "    seconds: 1726365405\n",
              "    nanos: 239452928\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 135\n",
              "  epoch: 4\n",
              "  mean_loss: 3.1256542205810547\n",
              "  compute_time {\n",
              "    seconds: 1726365405\n",
              "    nanos: 958164623\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 136\n",
              "  epoch: 4\n",
              "  mean_loss: 11.474260330200195\n",
              "  compute_time {\n",
              "    seconds: 1726365406\n",
              "    nanos: 847442602\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 137\n",
              "  epoch: 4\n",
              "  mean_loss: 7.100918292999268\n",
              "  compute_time {\n",
              "    seconds: 1726365407\n",
              "    nanos: 752528348\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 138\n",
              "  epoch: 4\n",
              "  mean_loss: 4.301681995391846\n",
              "  compute_time {\n",
              "    seconds: 1726365408\n",
              "    nanos: 612426454\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 139\n",
              "  epoch: 4\n",
              "  mean_loss: 4.9770894050598145\n",
              "  compute_time {\n",
              "    seconds: 1726365409\n",
              "    nanos: 275490023\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 140\n",
              "  epoch: 4\n",
              "  mean_loss: 11.366385459899902\n",
              "  compute_time {\n",
              "    seconds: 1726365410\n",
              "    nanos: 133243609\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 141\n",
              "  epoch: 4\n",
              "  mean_loss: 8.308369636535645\n",
              "  compute_time {\n",
              "    seconds: 1726365411\n",
              "    nanos: 63846757\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 142\n",
              "  epoch: 4\n",
              "  mean_loss: 11.556005477905273\n",
              "  compute_time {\n",
              "    seconds: 1726365411\n",
              "    nanos: 933456193\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 143\n",
              "  epoch: 5\n",
              "  mean_loss: 4.303539276123047\n",
              "  compute_time {\n",
              "    seconds: 1726365412\n",
              "    nanos: 719072065\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 144\n",
              "  epoch: 5\n",
              "  mean_loss: 2.2644572257995605\n",
              "  compute_time {\n",
              "    seconds: 1726365413\n",
              "    nanos: 627033747\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 145\n",
              "  epoch: 5\n",
              "  mean_loss: 3.2817797660827637\n",
              "  compute_time {\n",
              "    seconds: 1726365415\n",
              "    nanos: 609561741\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 146\n",
              "  epoch: 5\n",
              "  mean_loss: 1.9659417867660522\n",
              "  compute_time {\n",
              "    seconds: 1726365416\n",
              "    nanos: 583009148\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 147\n",
              "  epoch: 5\n",
              "  mean_loss: 2.6998486518859863\n",
              "  compute_time {\n",
              "    seconds: 1726365417\n",
              "    nanos: 416968786\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 148\n",
              "  epoch: 5\n",
              "  mean_loss: 6.103095531463623\n",
              "  compute_time {\n",
              "    seconds: 1726365418\n",
              "    nanos: 484092715\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 149\n",
              "  epoch: 5\n",
              "  mean_loss: 1.8030387163162231\n",
              "  compute_time {\n",
              "    seconds: 1726365419\n",
              "    nanos: 240826143\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 150\n",
              "  epoch: 5\n",
              "  mean_loss: 2.716301202774048\n",
              "  compute_time {\n",
              "    seconds: 1726365420\n",
              "    nanos: 279116567\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 151\n",
              "  epoch: 5\n",
              "  mean_loss: 10.087862014770508\n",
              "  compute_time {\n",
              "    seconds: 1726365421\n",
              "    nanos: 208425049\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 152\n",
              "  epoch: 5\n",
              "  mean_loss: 3.5373728275299072\n",
              "  compute_time {\n",
              "    seconds: 1726365422\n",
              "    nanos: 91689538\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 153\n",
              "  epoch: 5\n",
              "  mean_loss: 3.1585776805877686\n",
              "  compute_time {\n",
              "    seconds: 1726365423\n",
              "    nanos: 60827250\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 154\n",
              "  epoch: 5\n",
              "  mean_loss: 4.11476469039917\n",
              "  compute_time {\n",
              "    seconds: 1726365423\n",
              "    nanos: 739469234\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 155\n",
              "  epoch: 5\n",
              "  mean_loss: 13.890922546386719\n",
              "  compute_time {\n",
              "    seconds: 1726365424\n",
              "    nanos: 489034671\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 156\n",
              "  epoch: 5\n",
              "  mean_loss: 3.4909002780914307\n",
              "  compute_time {\n",
              "    seconds: 1726365425\n",
              "    nanos: 472004843\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 157\n",
              "  epoch: 5\n",
              "  mean_loss: 11.344953536987305\n",
              "  compute_time {\n",
              "    seconds: 1726365426\n",
              "    nanos: 379062827\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 158\n",
              "  epoch: 5\n",
              "  mean_loss: 3.211972713470459\n",
              "  compute_time {\n",
              "    seconds: 1726365427\n",
              "    nanos: 81172959\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 159\n",
              "  epoch: 5\n",
              "  mean_loss: 7.903629302978516\n",
              "  compute_time {\n",
              "    seconds: 1726365427\n",
              "    nanos: 978625080\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 160\n",
              "  epoch: 5\n",
              "  mean_loss: 1.8086328506469727\n",
              "  compute_time {\n",
              "    seconds: 1726365428\n",
              "    nanos: 887539104\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 161\n",
              "  epoch: 5\n",
              "  mean_loss: 2.603294849395752\n",
              "  compute_time {\n",
              "    seconds: 1726365429\n",
              "    nanos: 739999004\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 162\n",
              "  epoch: 5\n",
              "  mean_loss: 3.682265043258667\n",
              "  compute_time {\n",
              "    seconds: 1726365430\n",
              "    nanos: 642911592\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 163\n",
              "  epoch: 5\n",
              "  mean_loss: 9.879058837890625\n",
              "  compute_time {\n",
              "    seconds: 1726365431\n",
              "    nanos: 604942371\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 164\n",
              "  epoch: 5\n",
              "  mean_loss: 8.886772155761719\n",
              "  compute_time {\n",
              "    seconds: 1726365432\n",
              "    nanos: 518490327\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 165\n",
              "  epoch: 5\n",
              "  mean_loss: 7.378897666931152\n",
              "  compute_time {\n",
              "    seconds: 1726365433\n",
              "    nanos: 334135704\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 166\n",
              "  epoch: 5\n",
              "  mean_loss: 10.941515922546387\n",
              "  compute_time {\n",
              "    seconds: 1726365434\n",
              "    nanos: 137442943\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 167\n",
              "  epoch: 5\n",
              "  mean_loss: 8.340740203857422\n",
              "  compute_time {\n",
              "    seconds: 1726365435\n",
              "    nanos: 110374587\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 168\n",
              "  epoch: 5\n",
              "  mean_loss: 3.418092727661133\n",
              "  compute_time {\n",
              "    seconds: 1726365436\n",
              "    nanos: 45783083\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 169\n",
              "  epoch: 5\n",
              "  mean_loss: 10.268373489379883\n",
              "  compute_time {\n",
              "    seconds: 1726365436\n",
              "    nanos: 838083422\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 170\n",
              "  epoch: 5\n",
              "  mean_loss: 4.353221893310547\n",
              "  compute_time {\n",
              "    seconds: 1726365437\n",
              "    nanos: 787726938\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 171\n",
              "  epoch: 6\n",
              "  mean_loss: 3.4982364177703857\n",
              "  compute_time {\n",
              "    seconds: 1726365438\n",
              "    nanos: 469014819\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 172\n",
              "  epoch: 6\n",
              "  mean_loss: 1.1876658201217651\n",
              "  compute_time {\n",
              "    seconds: 1726365439\n",
              "    nanos: 152586754\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 173\n",
              "  epoch: 6\n",
              "  mean_loss: 1.7196369171142578\n",
              "  compute_time {\n",
              "    seconds: 1726365440\n",
              "    nanos: 71102380\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 174\n",
              "  epoch: 6\n",
              "  mean_loss: 1.629380226135254\n",
              "  compute_time {\n",
              "    seconds: 1726365440\n",
              "    nanos: 757102417\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 175\n",
              "  epoch: 6\n",
              "  mean_loss: 5.24246072769165\n",
              "  compute_time {\n",
              "    seconds: 1726365441\n",
              "    nanos: 513315819\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 176\n",
              "  epoch: 6\n",
              "  mean_loss: 4.2901611328125\n",
              "  compute_time {\n",
              "    seconds: 1726365442\n",
              "    nanos: 411534421\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 177\n",
              "  epoch: 6\n",
              "  mean_loss: 2.7479002475738525\n",
              "  compute_time {\n",
              "    seconds: 1726365443\n",
              "    nanos: 231788471\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 178\n",
              "  epoch: 6\n",
              "  mean_loss: 2.4929702281951904\n",
              "  compute_time {\n",
              "    seconds: 1726365443\n",
              "    nanos: 949798022\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 179\n",
              "  epoch: 6\n",
              "  mean_loss: 1.6079866886138916\n",
              "  compute_time {\n",
              "    seconds: 1726365444\n",
              "    nanos: 863848347\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 180\n",
              "  epoch: 6\n",
              "  mean_loss: 11.969942092895508\n",
              "  compute_time {\n",
              "    seconds: 1726365445\n",
              "    nanos: 747476133\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 181\n",
              "  epoch: 6\n",
              "  mean_loss: 7.232685089111328\n",
              "  compute_time {\n",
              "    seconds: 1726365446\n",
              "    nanos: 557763846\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 182\n",
              "  epoch: 6\n",
              "  mean_loss: 6.1738739013671875\n",
              "  compute_time {\n",
              "    seconds: 1726365447\n",
              "    nanos: 335081653\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 183\n",
              "  epoch: 6\n",
              "  mean_loss: 1.511531114578247\n",
              "  compute_time {\n",
              "    seconds: 1726365448\n",
              "    nanos: 83786317\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 184\n",
              "  epoch: 6\n",
              "  mean_loss: 5.353781700134277\n",
              "  compute_time {\n",
              "    seconds: 1726365448\n",
              "    nanos: 989578616\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 185\n",
              "  epoch: 6\n",
              "  mean_loss: 3.435410976409912\n",
              "  compute_time {\n",
              "    seconds: 1726365449\n",
              "    nanos: 886113716\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 186\n",
              "  epoch: 6\n",
              "  mean_loss: 2.026309013366699\n",
              "  compute_time {\n",
              "    seconds: 1726365450\n",
              "    nanos: 735882362\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 187\n",
              "  epoch: 6\n",
              "  mean_loss: 1.3688311576843262\n",
              "  compute_time {\n",
              "    seconds: 1726365451\n",
              "    nanos: 633297188\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 188\n",
              "  epoch: 6\n",
              "  mean_loss: 3.089902877807617\n",
              "  compute_time {\n",
              "    seconds: 1726365452\n",
              "    nanos: 526358043\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 189\n",
              "  epoch: 6\n",
              "  mean_loss: 5.011688709259033\n",
              "  compute_time {\n",
              "    seconds: 1726365453\n",
              "    nanos: 437326028\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 190\n",
              "  epoch: 6\n",
              "  mean_loss: 5.87611722946167\n",
              "  compute_time {\n",
              "    seconds: 1726365454\n",
              "    nanos: 81050996\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 191\n",
              "  epoch: 6\n",
              "  mean_loss: 2.87465238571167\n",
              "  compute_time {\n",
              "    seconds: 1726365454\n",
              "    nanos: 946945936\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 192\n",
              "  epoch: 6\n",
              "  mean_loss: 4.271553039550781\n",
              "  compute_time {\n",
              "    seconds: 1726365455\n",
              "    nanos: 842266021\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 193\n",
              "  epoch: 6\n",
              "  mean_loss: 1.070155143737793\n",
              "  compute_time {\n",
              "    seconds: 1726365456\n",
              "    nanos: 600681157\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 194\n",
              "  epoch: 6\n",
              "  mean_loss: 6.0662431716918945\n",
              "  compute_time {\n",
              "    seconds: 1726365457\n",
              "    nanos: 353812122\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 195\n",
              "  epoch: 6\n",
              "  mean_loss: 2.5277979373931885\n",
              "  compute_time {\n",
              "    seconds: 1726365458\n",
              "    nanos: 52891037\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 196\n",
              "  epoch: 6\n",
              "  mean_loss: 2.038241147994995\n",
              "  compute_time {\n",
              "    seconds: 1726365458\n",
              "    nanos: 924424669\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 197\n",
              "  epoch: 6\n",
              "  mean_loss: 5.232227325439453\n",
              "  compute_time {\n",
              "    seconds: 1726365459\n",
              "    nanos: 854537607\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 198\n",
              "  epoch: 6\n",
              "  mean_loss: 2.4335944652557373\n",
              "  compute_time {\n",
              "    seconds: 1726365460\n",
              "    nanos: 613969494\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 199\n",
              "  epoch: 6\n",
              "  mean_loss: 8.228487968444824\n",
              "  compute_time {\n",
              "    seconds: 1726365461\n",
              "    nanos: 464603728\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 200\n",
              "  epoch: 7\n",
              "  mean_loss: 6.555017471313477\n",
              "  compute_time {\n",
              "    seconds: 1726365462\n",
              "    nanos: 459627828\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 201\n",
              "  epoch: 7\n",
              "  mean_loss: 2.1363980770111084\n",
              "  compute_time {\n",
              "    seconds: 1726365463\n",
              "    nanos: 357723151\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 202\n",
              "  epoch: 7\n",
              "  mean_loss: 0.792794406414032\n",
              "  compute_time {\n",
              "    seconds: 1726365464\n",
              "    nanos: 275259334\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 203\n",
              "  epoch: 7\n",
              "  mean_loss: 6.427527904510498\n",
              "  compute_time {\n",
              "    seconds: 1726365465\n",
              "    nanos: 37980715\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 204\n",
              "  epoch: 7\n",
              "  mean_loss: 2.638321876525879\n",
              "  compute_time {\n",
              "    seconds: 1726365465\n",
              "    nanos: 973598517\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 205\n",
              "  epoch: 7\n",
              "  mean_loss: 1.7170445919036865\n",
              "  compute_time {\n",
              "    seconds: 1726365466\n",
              "    nanos: 784893275\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 206\n",
              "  epoch: 7\n",
              "  mean_loss: 1.8453929424285889\n",
              "  compute_time {\n",
              "    seconds: 1726365467\n",
              "    nanos: 706727503\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 207\n",
              "  epoch: 7\n",
              "  mean_loss: 1.9719226360321045\n",
              "  compute_time {\n",
              "    seconds: 1726365468\n",
              "    nanos: 472582601\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 208\n",
              "  epoch: 7\n",
              "  mean_loss: 3.20662260055542\n",
              "  compute_time {\n",
              "    seconds: 1726365469\n",
              "    nanos: 249881232\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 209\n",
              "  epoch: 7\n",
              "  mean_loss: 2.9295294284820557\n",
              "  compute_time {\n",
              "    seconds: 1726365470\n",
              "    nanos: 228038449\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 210\n",
              "  epoch: 7\n",
              "  mean_loss: 2.168440103530884\n",
              "  compute_time {\n",
              "    seconds: 1726365471\n",
              "    nanos: 168263131\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 211\n",
              "  epoch: 7\n",
              "  mean_loss: 2.3127570152282715\n",
              "  compute_time {\n",
              "    seconds: 1726365472\n",
              "    nanos: 38975787\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 212\n",
              "  epoch: 7\n",
              "  mean_loss: 2.0731327533721924\n",
              "  compute_time {\n",
              "    seconds: 1726365472\n",
              "    nanos: 832897621\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 213\n",
              "  epoch: 7\n",
              "  mean_loss: 2.3103556632995605\n",
              "  compute_time {\n",
              "    seconds: 1726365473\n",
              "    nanos: 514748072\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 214\n",
              "  epoch: 7\n",
              "  mean_loss: 8.69166374206543\n",
              "  compute_time {\n",
              "    seconds: 1726365474\n",
              "    nanos: 414008951\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 215\n",
              "  epoch: 7\n",
              "  mean_loss: 3.49124813079834\n",
              "  compute_time {\n",
              "    seconds: 1726365475\n",
              "    nanos: 969939214\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 216\n",
              "  epoch: 7\n",
              "  mean_loss: 1.0447465181350708\n",
              "  compute_time {\n",
              "    seconds: 1726365476\n",
              "    nanos: 956853952\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 217\n",
              "  epoch: 7\n",
              "  mean_loss: 1.291494607925415\n",
              "  compute_time {\n",
              "    seconds: 1726365477\n",
              "    nanos: 812660476\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 218\n",
              "  epoch: 7\n",
              "  mean_loss: 5.160313606262207\n",
              "  compute_time {\n",
              "    seconds: 1726365478\n",
              "    nanos: 790463927\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 219\n",
              "  epoch: 7\n",
              "  mean_loss: 1.3785558938980103\n",
              "  compute_time {\n",
              "    seconds: 1726365479\n",
              "    nanos: 523552405\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 220\n",
              "  epoch: 7\n",
              "  mean_loss: 3.3040149211883545\n",
              "  compute_time {\n",
              "    seconds: 1726365480\n",
              "    nanos: 196151585\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 221\n",
              "  epoch: 7\n",
              "  mean_loss: 2.8829047679901123\n",
              "  compute_time {\n",
              "    seconds: 1726365480\n",
              "    nanos: 931210006\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 222\n",
              "  epoch: 7\n",
              "  mean_loss: 0.6518465876579285\n",
              "  compute_time {\n",
              "    seconds: 1726365481\n",
              "    nanos: 874674674\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 223\n",
              "  epoch: 7\n",
              "  mean_loss: 3.1410417556762695\n",
              "  compute_time {\n",
              "    seconds: 1726365482\n",
              "    nanos: 675120300\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 224\n",
              "  epoch: 7\n",
              "  mean_loss: 9.695626258850098\n",
              "  compute_time {\n",
              "    seconds: 1726365483\n",
              "    nanos: 609944919\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 225\n",
              "  epoch: 7\n",
              "  mean_loss: 3.5265538692474365\n",
              "  compute_time {\n",
              "    seconds: 1726365484\n",
              "    nanos: 403799934\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 226\n",
              "  epoch: 7\n",
              "  mean_loss: 3.5136005878448486\n",
              "  compute_time {\n",
              "    seconds: 1726365485\n",
              "    nanos: 326830398\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 227\n",
              "  epoch: 7\n",
              "  mean_loss: 1.7922945022583008\n",
              "  compute_time {\n",
              "    seconds: 1726365486\n",
              "    nanos: 276625476\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 228\n",
              "  epoch: 8\n",
              "  mean_loss: 5.070186614990234\n",
              "  compute_time {\n",
              "    seconds: 1726365487\n",
              "    nanos: 209204742\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 229\n",
              "  epoch: 8\n",
              "  mean_loss: 1.3127261400222778\n",
              "  compute_time {\n",
              "    seconds: 1726365487\n",
              "    nanos: 906099154\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 230\n",
              "  epoch: 8\n",
              "  mean_loss: 1.6589092016220093\n",
              "  compute_time {\n",
              "    seconds: 1726365488\n",
              "    nanos: 944339611\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 231\n",
              "  epoch: 8\n",
              "  mean_loss: 0.711771547794342\n",
              "  compute_time {\n",
              "    seconds: 1726365489\n",
              "    nanos: 871329304\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 232\n",
              "  epoch: 8\n",
              "  mean_loss: 2.2745413780212402\n",
              "  compute_time {\n",
              "    seconds: 1726365490\n",
              "    nanos: 810910643\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 233\n",
              "  epoch: 8\n",
              "  mean_loss: 1.6375088691711426\n",
              "  compute_time {\n",
              "    seconds: 1726365491\n",
              "    nanos: 586932121\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 234\n",
              "  epoch: 8\n",
              "  mean_loss: 1.942491888999939\n",
              "  compute_time {\n",
              "    seconds: 1726365492\n",
              "    nanos: 522079966\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 235\n",
              "  epoch: 8\n",
              "  mean_loss: 3.202589511871338\n",
              "  compute_time {\n",
              "    seconds: 1726365493\n",
              "    nanos: 435199926\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 236\n",
              "  epoch: 8\n",
              "  mean_loss: 3.6608214378356934\n",
              "  compute_time {\n",
              "    seconds: 1726365494\n",
              "    nanos: 359006541\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 237\n",
              "  epoch: 8\n",
              "  mean_loss: 0.8877213001251221\n",
              "  compute_time {\n",
              "    seconds: 1726365495\n",
              "    nanos: 310244665\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 238\n",
              "  epoch: 8\n",
              "  mean_loss: 2.2450332641601562\n",
              "  compute_time {\n",
              "    seconds: 1726365496\n",
              "    nanos: 154792857\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 239\n",
              "  epoch: 8\n",
              "  mean_loss: 4.371181964874268\n",
              "  compute_time {\n",
              "    seconds: 1726365497\n",
              "    nanos: 107069668\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 240\n",
              "  epoch: 8\n",
              "  mean_loss: 3.3917558193206787\n",
              "  compute_time {\n",
              "    seconds: 1726365497\n",
              "    nanos: 962587547\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 241\n",
              "  epoch: 8\n",
              "  mean_loss: 2.416078567504883\n",
              "  compute_time {\n",
              "    seconds: 1726365498\n",
              "    nanos: 683805099\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 242\n",
              "  epoch: 8\n",
              "  mean_loss: 1.3939350843429565\n",
              "  compute_time {\n",
              "    seconds: 1726365499\n",
              "    nanos: 397498892\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 243\n",
              "  epoch: 8\n",
              "  mean_loss: 5.824974060058594\n",
              "  compute_time {\n",
              "    seconds: 1726365500\n",
              "    nanos: 387498520\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 244\n",
              "  epoch: 8\n",
              "  mean_loss: 2.692145347595215\n",
              "  compute_time {\n",
              "    seconds: 1726365501\n",
              "    nanos: 35703233\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 245\n",
              "  epoch: 8\n",
              "  mean_loss: 0.8742115497589111\n",
              "  compute_time {\n",
              "    seconds: 1726365501\n",
              "    nanos: 703950273\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 246\n",
              "  epoch: 8\n",
              "  mean_loss: 3.387115001678467\n",
              "  compute_time {\n",
              "    seconds: 1726365502\n",
              "    nanos: 361182350\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 247\n",
              "  epoch: 8\n",
              "  mean_loss: 5.351202964782715\n",
              "  compute_time {\n",
              "    seconds: 1726365503\n",
              "    nanos: 196609854\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 248\n",
              "  epoch: 8\n",
              "  mean_loss: 2.057713508605957\n",
              "  compute_time {\n",
              "    seconds: 1726365504\n",
              "    nanos: 124803031\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 249\n",
              "  epoch: 8\n",
              "  mean_loss: 1.1076573133468628\n",
              "  compute_time {\n",
              "    seconds: 1726365504\n",
              "    nanos: 976362578\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 250\n",
              "  epoch: 8\n",
              "  mean_loss: 1.174384593963623\n",
              "  compute_time {\n",
              "    seconds: 1726365505\n",
              "    nanos: 909488293\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 251\n",
              "  epoch: 8\n",
              "  mean_loss: 0.3054043650627136\n",
              "  compute_time {\n",
              "    seconds: 1726365507\n",
              "    nanos: 457568761\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 252\n",
              "  epoch: 8\n",
              "  mean_loss: 3.9665098190307617\n",
              "  compute_time {\n",
              "    seconds: 1726365508\n",
              "    nanos: 96287297\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 253\n",
              "  epoch: 8\n",
              "  mean_loss: 4.095708847045898\n",
              "  compute_time {\n",
              "    seconds: 1726365509\n",
              "    nanos: 21672423\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 254\n",
              "  epoch: 8\n",
              "  mean_loss: 5.940491676330566\n",
              "  compute_time {\n",
              "    seconds: 1726365509\n",
              "    nanos: 947007248\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 255\n",
              "  epoch: 8\n",
              "  mean_loss: 2.4911577701568604\n",
              "  compute_time {\n",
              "    seconds: 1726365510\n",
              "    nanos: 786853154\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 256\n",
              "  epoch: 8\n",
              "  mean_loss: 1.9060364961624146\n",
              "  compute_time {\n",
              "    seconds: 1726365511\n",
              "    nanos: 469372889\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 257\n",
              "  epoch: 9\n",
              "  mean_loss: 0.6726310849189758\n",
              "  compute_time {\n",
              "    seconds: 1726365512\n",
              "    nanos: 412721424\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 258\n",
              "  epoch: 9\n",
              "  mean_loss: 1.2170193195343018\n",
              "  compute_time {\n",
              "    seconds: 1726365513\n",
              "    nanos: 214378534\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 259\n",
              "  epoch: 9\n",
              "  mean_loss: 1.0421944856643677\n",
              "  compute_time {\n",
              "    seconds: 1726365514\n",
              "    nanos: 169865758\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 260\n",
              "  epoch: 9\n",
              "  mean_loss: 3.2173585891723633\n",
              "  compute_time {\n",
              "    seconds: 1726365514\n",
              "    nanos: 878328784\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 261\n",
              "  epoch: 9\n",
              "  mean_loss: 2.731510877609253\n",
              "  compute_time {\n",
              "    seconds: 1726365515\n",
              "    nanos: 799899618\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 262\n",
              "  epoch: 9\n",
              "  mean_loss: 0.7639212608337402\n",
              "  compute_time {\n",
              "    seconds: 1726365516\n",
              "    nanos: 724391765\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 263\n",
              "  epoch: 9\n",
              "  mean_loss: 0.9456429481506348\n",
              "  compute_time {\n",
              "    seconds: 1726365517\n",
              "    nanos: 518364848\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 264\n",
              "  epoch: 9\n",
              "  mean_loss: 2.589620590209961\n",
              "  compute_time {\n",
              "    seconds: 1726365518\n",
              "    nanos: 330739330\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 265\n",
              "  epoch: 9\n",
              "  mean_loss: 4.055851936340332\n",
              "  compute_time {\n",
              "    seconds: 1726365519\n",
              "    nanos: 133006720\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 266\n",
              "  epoch: 9\n",
              "  mean_loss: 3.024369716644287\n",
              "  compute_time {\n",
              "    seconds: 1726365519\n",
              "    nanos: 789065909\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 267\n",
              "  epoch: 9\n",
              "  mean_loss: 1.5094887018203735\n",
              "  compute_time {\n",
              "    seconds: 1726365520\n",
              "    nanos: 696460976\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 268\n",
              "  epoch: 9\n",
              "  mean_loss: 2.1658406257629395\n",
              "  compute_time {\n",
              "    seconds: 1726365521\n",
              "    nanos: 573383782\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 269\n",
              "  epoch: 9\n",
              "  mean_loss: 2.7236850261688232\n",
              "  compute_time {\n",
              "    seconds: 1726365522\n",
              "    nanos: 470913016\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 270\n",
              "  epoch: 9\n",
              "  mean_loss: 1.4699592590332031\n",
              "  compute_time {\n",
              "    seconds: 1726365523\n",
              "    nanos: 382548474\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 271\n",
              "  epoch: 9\n",
              "  mean_loss: 2.9964370727539062\n",
              "  compute_time {\n",
              "    seconds: 1726365524\n",
              "    nanos: 248894532\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 272\n",
              "  epoch: 9\n",
              "  mean_loss: 1.5059291124343872\n",
              "  compute_time {\n",
              "    seconds: 1726365525\n",
              "    nanos: 140706085\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 273\n",
              "  epoch: 9\n",
              "  mean_loss: 0.9746024012565613\n",
              "  compute_time {\n",
              "    seconds: 1726365525\n",
              "    nanos: 816731848\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 274\n",
              "  epoch: 9\n",
              "  mean_loss: 0.34263575077056885\n",
              "  compute_time {\n",
              "    seconds: 1726365526\n",
              "    nanos: 724274561\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 275\n",
              "  epoch: 9\n",
              "  mean_loss: 1.2590054273605347\n",
              "  compute_time {\n",
              "    seconds: 1726365527\n",
              "    nanos: 550824502\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 276\n",
              "  epoch: 9\n",
              "  mean_loss: 2.63700270652771\n",
              "  compute_time {\n",
              "    seconds: 1726365528\n",
              "    nanos: 411554993\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 277\n",
              "  epoch: 9\n",
              "  mean_loss: 2.256251335144043\n",
              "  compute_time {\n",
              "    seconds: 1726365529\n",
              "    nanos: 327766182\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 278\n",
              "  epoch: 9\n",
              "  mean_loss: 2.4356822967529297\n",
              "  compute_time {\n",
              "    seconds: 1726365530\n",
              "    nanos: 83746293\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 279\n",
              "  epoch: 9\n",
              "  mean_loss: 10.31749153137207\n",
              "  compute_time {\n",
              "    seconds: 1726365530\n",
              "    nanos: 784388009\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 280\n",
              "  epoch: 9\n",
              "  mean_loss: 0.8714662790298462\n",
              "  compute_time {\n",
              "    seconds: 1726365531\n",
              "    nanos: 506420561\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 281\n",
              "  epoch: 9\n",
              "  mean_loss: 6.068657398223877\n",
              "  compute_time {\n",
              "    seconds: 1726365532\n",
              "    nanos: 372187511\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 282\n",
              "  epoch: 9\n",
              "  mean_loss: 1.9093812704086304\n",
              "  compute_time {\n",
              "    seconds: 1726365533\n",
              "    nanos: 49769688\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 283\n",
              "  epoch: 9\n",
              "  mean_loss: 0.5839450359344482\n",
              "  compute_time {\n",
              "    seconds: 1726365533\n",
              "    nanos: 802242820\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 284\n",
              "  epoch: 9\n",
              "  mean_loss: 3.1414778232574463\n",
              "  compute_time {\n",
              "    seconds: 1726365534\n",
              "    nanos: 753273299\n",
              "  }\n",
              "}\n",
              "snapshots {\n",
              "  step: 285\n",
              "  epoch: 10\n",
              "  mean_loss: 2.2105612754821777\n",
              "  compute_time {\n",
              "    seconds: 1726365536\n",
              "    nanos: 928331890\n",
              "  }\n",
              "}\n",
              "tuned_model: \"tunedModels/finetuninggemmafordl1-xxcubsl6ftaf\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "operation.metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P51-74Q9bqBz",
        "outputId": "4951ba3d-560c-4fc9-a729-c9cc92a5a59e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TunedModel(name='tunedModels/finetuninggemmafordl1-xxcubsl6ftaf',\n",
              "           source_model='models/gemini-1.0-pro-001',\n",
              "           base_model='models/gemini-1.0-pro-001',\n",
              "           display_name='fineTuning-gemmafordl-1',\n",
              "           description='',\n",
              "           temperature=0.3,\n",
              "           top_p=1.0,\n",
              "           top_k=0,\n",
              "           state=<State.ACTIVE: 2>,\n",
              "           create_time=datetime.datetime(2024, 9, 15, 1, 54, 50, 823359, tzinfo=datetime.timezone.utc),\n",
              "           update_time=datetime.datetime(2024, 9, 15, 1, 58, 57, 63484, tzinfo=datetime.timezone.utc),\n",
              "           tuning_task=TuningTask(start_time=datetime.datetime(2024, 9, 15, 1, 54, 51, 90125, tzinfo=datetime.timezone.utc),\n",
              "                                  complete_time=datetime.datetime(2024, 9, 15, 1, 58, 57, 63484, tzinfo=datetime.timezone.utc),\n",
              "                                  snapshots=[...],\n",
              "                                  hyperparameters=Hyperparameters(epoch_count=10,\n",
              "                                                                  batch_size=4,\n",
              "                                                                  learning_rate=0.001)))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.get_tuned_model('tunedModels/finetuninggemmafordl1-xxcubsl6ftaf')\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h-Su4qDT3NO1"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model = genai.GenerativeModel(model_name=model.name)\n",
        "result = fine_tuned_model.generate_content(\"Explain me about the Sequential Api in keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qMwnqAjyepv",
        "outputId": "3dca71e6-dcc2-4a00-e752-5ef3bce983a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Sequential API in Keras groups a linear stack of layers into a Model. It provides a simple and intuitive way to build models where layers are added sequentially. Here's an example:\n",
            "\n",
            "```python\n",
            "model = keras.Sequential([\n",
            "    keras.Input(shape=(None, None, 3)),\n",
            "    keras.layers.Conv2D(filters=32, kernel_size=3),\n",
            "    keras.layers.MaxPooling2D(),\n",
            "])\n",
            "```\n",
            "The Sequential API is best suited for models with a straightforward layer stacking pattern. For more complex architectures, consider using the Functional API or the Model subclassing approach.\n"
          ]
        }
      ],
      "source": [
        "print(result.text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09b2b43f17ae462796d5d91b24b1d93f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "203912e07c5f4640aa0ba9e16192bece": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22dc53d04e8f4087a9a9b9ea47c7b309": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_203912e07c5f4640aa0ba9e16192bece",
            "placeholder": "",
            "style": "IPY_MODEL_e58fa538fb434f8e9492526beb031b59",
            "value": "100%"
          }
        },
        "33d6d095901d455bbed74c429a3f33f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09b2b43f17ae462796d5d91b24b1d93f",
            "placeholder": "",
            "style": "IPY_MODEL_8e7187ce8eb34f70a34858af50f4cdaf",
            "value": "285/285[04:14&lt;00:00,1.28it/s]"
          }
        },
        "4a0aac949f864bdd8f7b7d16681c2941": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886062f309834c9d969eeae3608a3d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e7187ce8eb34f70a34858af50f4cdaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7781125c7b6418292524071c36bd91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22dc53d04e8f4087a9a9b9ea47c7b309",
              "IPY_MODEL_b98e13523b7f438daac143f1991e2455",
              "IPY_MODEL_33d6d095901d455bbed74c429a3f33f6"
            ],
            "layout": "IPY_MODEL_cd31641901ce4866824b4d88e09d83e9"
          }
        },
        "b98e13523b7f438daac143f1991e2455": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a0aac949f864bdd8f7b7d16681c2941",
            "max": 285,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_886062f309834c9d969eeae3608a3d0f",
            "value": 285
          }
        },
        "cd31641901ce4866824b4d88e09d83e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e58fa538fb434f8e9492526beb031b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
